<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="MioKinHuang &#39;s Blog">
<meta property="og:url" content="https://huangmiaoxin.github.io/index.html">
<meta property="og:site_name" content="MioKinHuang &#39;s Blog">
<meta property="article:author" content="MioKinHuang">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://huangmiaoxin.github.io/"/>





  <title>MioKinHuang 's Blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MioKinHuang 's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/" itemprop="url">极大似然估计和最大后验概率估计的深入理解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T21:33:58+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>进行贝叶斯估计的时候，我们利用贝叶斯公式将$P(class|x)$的问题转化成了$P(x|class)P(class)$的问题。其中类条件概率$ P(x|class)$的估计是重点。<br>$P(x|class)$由参数$\theta$确定，则类条件转化成求取$P(x|\theta)$</p>
<p>关于$P(x|\theta)$的求解，从上世纪二三十年代开始就有争论。主要流派为频率主义学派和贝叶斯学派。<br>频率主义学派认为参数估计中，参数未知，但是一个固定的值，通过优化似然函数可以求解。<br>贝叶斯学派认为参数未知，而且是一个符合某种分布的变量，可以假定参数服从一个先验分布，然后通过观测训练集，获得参数的后验分布。<br>这两种分别对应下文的两种方法。</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>平时我们经常用到该方法。训练模型的时候假定模型参数，然后计算出每个样本的概率，依照最大似然的原则，优化训练集样本乘积为最大，即$P(X|\theta)$为最大，由此求取参数值。</p>
<h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h3><p>最大后验概率则认为参数本身存在一个先验分布，故优化目标不仅仅是似然函数，因为参数先验分布是前提，故优化目标是参数先验分布乘上似然函数，其实非常好理解。<br>这部分也就是贝叶斯公式右边的分子部分，故称为后验概率。即优化$P(\theta|X)$为最大。<br>贝叶斯学派主张优化后验概率至最大，由此求得参数θ。</p>
<p><strong>举个例子：假设现在抛了5次硬币，有4次朝上，1次朝下，则第6次硬币朝上的概率？</strong></p>
<p><strong>极大似然估计：</strong><br>按照理解，直觉想即是，4/5<br>追究具体过程的话可以是：<br>假定朝上概率是$\theta$，则极大似然概率$P=\theta^4(1-\theta)$，寻找$\theta$使得P最大，简单可知$\theta=0.8$时P最大。</p>
<p><strong>最大后验估计：</strong></p>
<ol>
<li>按照先验，人主观认为硬币是均匀的，则朝上概率最可能是0.5。此处如果我们选定先验的函数形式是高斯函数，则0.5应该定为均值，符合$\theta$最可能是0.5的先验。<br>则：<br>$P(\theta)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(\theta-0.5)^2}{2\sigma^2}}$<br>暂时取定$\sigma=0.1$<br>优化$P(X|\theta)P(\theta)$为最大，则是等价于优化$C_5^4\theta^4(1-\theta)P(\theta)$，借用matlab画出该曲线，可以看出$\theta$取值大约是0.7<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-12.png" class="">
此时对比极大似然估计可以看出，先验本来是0.5，但是发生的5次抛硬币事件使得先验得到修正，朝着0.8的方向修正，变成了0.7，假如试验的次数继续增多，例如增加到500次，其中400次朝上，则如下图：<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-26.png" class="">
可以看出，多次的事实最终使得先验被彻底修正，$\theta$取值无限逼近0.8。<br>此时如果我们再想深入一步：假如抛了很多次硬币，发现正反的次数相等，这个高斯分布的假设能否求出$\theta=0.5$呢，即$C_{2M}^M\theta^M(1-\theta)^MP(\theta)$不管M取值多少，最大值都是在$\theta=0.5$取得吗？明显很难有这么巧的事情，例如M=5时，图像如下，最大值不是在$\theta=0.5$处取得，而且大体的规律是M越小最大值偏离0.5越多。<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-41.png" class="">
这说明用高斯函数的形式来拟合参数$\theta$的分布，这种做法本身就不自洽。</li>
<li>我们这里暂时先开一个上帝视角，直接引入一个二项分布。二项分布问题现在普遍采用的参数先验是Beta分布，该分布不存在上面说的自相矛盾的问题。<br>$ Beta(\alpha, \beta)=\frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha, \beta)}$中$\alpha,\beta$只要等比例增加，则Beta曲线的最高点横坐标依然不变。<br>按照前人的总结，Beta分布式二项分布的共轭先验，该分布可以使得先验概率和和后验概率拥有相同的分布形式，即称为共轭。(百度百科对‘共轭’这个词条的解释：两头牛背上的架子称为轭，轭使两头牛同步行走。共轭即为按一定的规律相配的一对。 <strong>通俗点说就是孪生。</strong> )<br>那现在就有两个问题：<br>a.使用Beta分布的形式做参数$\theta$的先验是否就客观。<br>b.是否还存在其他形式的共轭先验分布。<br>关于这两个问题，按照自己的理解，尝试解答一下：<br>假如我们事先并不知道参数的分布，暂定为均匀分布，则后验概率：<br>$P(\theta|x)=P(x|\theta)=\frac{C_n^x\theta^x(1-\theta)^{n-x}}{B(x)}$<br>其中分母是为了归一化，保证概率密度函数的积分为1.<br>由此可以看出，后验概率的形式是Beta分布的形式。同时，考虑到此处的后验概率会成为下一次计算的先验概率(例如，下个回合继续抛10次硬币，是6次朝上4次朝下，则分两个回合计算和两个回合一同计算的结果应该是一样的。)，由此推导出先验分布应该和后验分布是同一个分布。因此，先验分布必须是Beta分布。<br>理清楚了这些，回到刚刚的例子，现在使用Beta分布来完成最大后验概率计算。<br>首先确定先验是$Beta(\alpha, \beta)$，发生了4次朝上，1次朝下的实验之后，后验变成了$Beta(\alpha +4, \beta +1)$，<br>一般而言，我们预想中的硬币是均匀的，即$\alpha = \beta$，故先验中，硬币朝上的概率是$P=\frac{\alpha}{\alpha + \beta}=\frac{1}{2}$，后验中，由于发生了5次实验，增加了信息，硬币朝上概率得到修正，修正为$P’=f(\alpha)=\frac{\alpha +4}{\alpha + \beta +5}=\frac{\alpha +4}{2\alpha +5}$，可以看到，明显的：<br>$f(\alpha) , \alpha ∈ Z$ 是单调递减的，同时$0.5 &lt;  f(\alpha) &lt;= 0.8$，左右端点分别在$\alpha$取值为$\infty$和0时取得。由此可见，最终的结果还取决于$\alpha$的大小，实际上，按照上文所言，先验是上一步计算的后验，所以，这里的所谓Beta先验，其实也可以视为是从0先验，然后进行了$\alpha + \beta$次实验，得到了$\alpha$次朝上和$\beta$次朝下的实验结果，然后获取到的后验知识。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>理解极大似然和最大后验的区别和联系，需要理解好先验和共轭先验。最后继续用上面的例子做一个总结：那硬币的例子来说，我们出生的时候，对硬币一无所知，即$Beta(0,0)$，随后一系列的生活经验，告诉我们$Beta(\alpha, \beta)$，这里的$\alpha$和$\beta$按照生活经验是基本相等。同时，$f(\alpha)$是一个递减函数也很好理解，假如我们从小看到的抛硬币实验数量太多而且都是接近0.5的结果的话，本次实验区区5次的结果更容易被认为是巧合，难以撼动之前的经验，而如果之前见过的抛硬币实验极少，对”硬币”这种物品没什么概念的话，则本次的5次实验很容易让人接受”硬币”其实就是一种朝上概率接近0.8的物品。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/" itemprop="url">高维场景下的维度灾难与高维统计方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T21:09:41+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是关于高维情况下的一些场景特征。主要是从两个论文入手介绍高维空间的一些特点。本质上，神经网络的参数空间极为庞大，也拥有高维空间的一些特点和问题。本文第一篇论文阅读其实不算高维问题，就算一个引入，着重介绍第二篇论文。</p>
<h5 id="1-High-dimensional-Feature-and-Its-Efficient-compression-for-face-verification"><a href="#1-High-dimensional-Feature-and-Its-Efficient-compression-for-face-verification" class="headerlink" title="1. High-dimensional Feature and Its Efficient compression for face verification"></a>1. High-dimensional Feature and Its Efficient compression for face verification</h5><p>CVPR2013，微软孙剑组的人脸识别论文，主要提出了一种获取高维的特征描述子的方法，并对描述子进行降维，便于计算特征点的相似度。其实这不太像严格意义的高维统计。<br>算法：<br>文章从人脸识别的基本步骤入手：提取特征描述子和根据特征计算人脸相似度。从提取描述子入手，按照经验，越多的特征点可以使效果提高，使用现有的成果，dense facial lankmark with alignment，外加多尺度的金字塔变换得到的描述子，一起拼接起来，得到一个高维的描述子(100k dimension)，接下来需要进行降维和并计算向量相似度。<br>降维分为两个步骤：</p>
<ol>
<li>先对描述子$P$进行PCA，去除部分噪声干扰，得到$P$；</li>
<li>对$Y$进行进一步降维，因为需要存储一个很大的降维矩阵，为了test阶段的存储和计算方便，人工约束这个转换矩阵稀疏。因此，得到优化目标：<script type="math/tex; mode=display">
min_B |Y-B^TX|_2^2+\lambda |B|</script>可以看到，优化目标分为两个，一个是重构误差，一个是降维矩阵B的稀疏约束，可以视为一个正则化。<br>上式要求降维矩阵的稀疏性，这个约束可能是过强的。另外，实际上，由于常用的欧拉距离，余弦距离等距离度量方式都有旋转不变性，故可以考虑在$Y$上加多一些自由度，即加上一个旋转矩阵，增多一些可能性，对冲矩阵$B$带来的约束。上式变为：<script type="math/tex; mode=display">
min_B |R^TY-B^TX|_2^2+\lambda |B|</script>上式相比于1式，增加多了一个旋转自由度，在原paper中自称为Rotated Sparse Regression。</li>
</ol>
<p>求解：<br>求解这个目标函数。使用梯度下降的普遍解法进行优化可能也会得到结果，但结合这个具体的解析式分析一下，可以得到更优的解法。首先，在旋转矩阵R或者降维矩阵B固定其中之一的时候，目标函数是凸函数。故可以考虑对R和B异步优化，即：</p>
<ol>
<li>固定旋转矩阵R，优化降维矩阵B，初始化时R初始为单位阵I即可，直接用普遍的优化方式进行优化；</li>
<li>固定降维矩阵B，优化旋转矩阵R，此时优化目标如下：<script type="math/tex; mode=display">
min_R |R^TY-B^TX|_2^2</script>这个式子有闭式解，对$YX^TB$进行SVD，得到$UDV^T$，便得到$R=UV^T$。</li>
</ol>
<p>重复步骤1，2最终得到优化解。</p>
<h5 id="2-Dimensionality’s-Blessing-Clustering-Images-by-Underlying-Distribution"><a href="#2-Dimensionality’s-Blessing-Clustering-Images-by-Underlying-Distribution" class="headerlink" title="2. Dimensionality’s Blessing: Clustering Images by Underlying Distribution"></a>2. Dimensionality’s Blessing: Clustering Images by Underlying Distribution</h5><p>这个paper在cvpr一众基于深度网络的方法中显得非常与众不同，使用的方法属于 <strong>高维统计(high-dimensional statistics)</strong> 。总体来说，作者推导了一个根据图像的潜在分布来判断图像是否相似，进而完成聚类的算法。<br>图像聚类，不辅助外部监督信息的情况下，严格来说目前学术界做出来的效果其实没有很好，一些相同的场景在转换了相机的倾角之后，得到的图像甚至都会被认为是不同的景物。<br>考虑从思考图像的本质入手。<br>首先，图像来自于相机，产生一个图像的因素包括，相机的在地球中的位置，相机的倾角，天气条件，阳光的猛烈程度，相机中传感器的噪声等等，这些因素视为随机变量，他们各自拥有一个随机分布，从这些分布中采样，综合得到一张具体的图像，故图像是这些分布中的一个具体实例。<br>作者观点是，现在直接将图像进行聚类，或者降维后聚类，或者抽取特征后进行聚类的这种做法，难以做的很好是因为：<br>a. 本质上，得到一种图像的多种分布采样出来的实例，是无法判断出每个量从哪个分布而来，影响程度多少。换句话说，产生图像数据的这些分布其实是重叠的，即数据是chaotic的(chaotic: chaotic is defined as having distributions whose mean separation is significantly smaller than their standard deviation)。如果增大feature的维度，可以减缓这种混合的影响，但是又会导致另一个问题b；<br>b. contrast-loss，即如果一个向量空间维度足够大，则向量空间中的任意两个点的距离是趋向于一个常数值的(part of the curse of dimensionality)，这个可能有点违背直觉。由于高维空间的这个特性，如果为了避免问题a而增大向量维度，则问题可能会朝b方向恶化；</p>
<p>作者的观点是，应当增大向量空间的维度，但是可以避开问题b。相反地，可以利用高维空间的一些特性(所以题目是blessing of dimensionality)。</p>
<p>首先看看需要利用到的高维空间的一些特性。<br>其实高维空间有一些很违背直觉的特性。之所以违背直觉是因为我们生活在三维世界里，习惯了想象一些二维或者三维的物体，而且，超过4维的世界很难直观地可视化出来。例如，在二维的平面世界中，我们直观地看到，一个正方形除去内接圆的部分，占整个正方形面积的比重比内接圆占的比重要小，在三维的立体世界中也是如此，正方体除去求解球的部分，占整个正方体体积的比重比内接球占的比重要小。但实际上，可以定量的计算出，随着二维变到三维，这个残余部分的比重在增大。实际上，这种’面积体积’的概念可以推广到d维，此时，超球体和超立方体的’超体积’的比值计算如下：</p>
<script type="math/tex; mode=display">
\frac{2r^d \pi^{d/2}}{d\Gamma (d/2)} = \frac{\pi^{d/2}}{d 2^{d-1}\Gamma (d/2)}</script><p>从这里可以看到，当d越来越大，这两个超体积的比值变得越来越小，当d趋向于无穷时，这个比值变成了0。这说明，在高维空间里，内接的超球体只占了整个空间的非常微小的一部分，这反映了高维空间实际上非常之空阔。</p>
<p>在paper中，作者的说法是，在一个高维空间中，一个超球体的超体积随半径的变化是非常剧烈的，如下：</p>
<script type="math/tex; mode=display">
(\frac{r-\Delta r}{r})^k = (1-\frac{\Delta r}{r})^k \rightarrow 0, k \rightarrow \infty</script><p>故维度足够高的话，超球体的超体积其实都是超球体的最外层贡献的，几乎和内部无关。因此，如果从这样一个高维超球体范围内中采样一个点，该点基本就是落在球体的表面而已，因为内部空间小到可以忽略，只有无穷小的概率会采样到内部点。关于这个，作者尝试给了一个可视化图，如下：<br><img src="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/2020-02-06-21-17-55.png" class=""><br>从上面结论出发，作者进一步说明了，如果构成图像的所有潜在分布的维度足够高，那么从中采样一个示例出来，其实就是落在这些分布构成的超球体的表面极其‘稀薄’的一圈圆环而已，从这一点来看，一些重叠的高维分布的实例，其实并没有发生混合。示意图如下：<br><img src="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/2020-02-06-21-18-30.png" class=""><br>另外，因为图像的像素局部是相关的，较远距离的像素是无关的，故无法将像素之间都当做严格独立的， 于是作者又尝试说明了‘准独立’(quasi-independent)的一些特性，即在无限高维的向量中，只有有限维度的向量之间是不独立的情况。</p>
<p>经过一番推理之后，得出结论，在准独立的情况下，高维空间中，两个向量之间的距离几乎取决于所在分布的均值和方差，和向量具体的值无关。所以，对图像聚类，需要判断两个图像向量之间的距离，转化为识别两个图像潜在分布的均值和方差。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/" itemprop="url">论文阅读：基于对抗网络的无监督生成式文本摘要</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T17:40:50+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是对ICLR2018的论文 <strong>《Learning to Encode Text as Human-Readable Summaries using generative adversarial netword》</strong> 的阅读笔记，总结了一下论文的做法，思考了一些可能存在的问题，也分析一下后续可能的技术借鉴和迁移。</p>
<h3 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h3><p>autoencoder常常被用来编码和压缩信息。例如在句子表示中，可以将句子压缩成一个稠密向量，表示空间中某个具体的点。但是，这个表示方法人是无法感知的。作者借鉴autoencoder的思想，提出了一种新的思路，直接将长文本压缩成短文本。这个过程可以完全使用无监督方法。</p>
<h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h3><p>从动机出发，考虑将长文本先经过一个seq2seq模型，压缩为一个短文本，这里的seq2seq模型称为Generator，此处的监督信息来自两部分：</p>
<ol>
<li>短文本经过另一个seq2seq模型，能恢复为长文本，此处的seq2seq模型称为Reconstructor；</li>
<li>很明显，仅仅是重构约束的话，Generator和Reconstructor之间可能创造了另一种奇怪的语言，导致生成的短文本虽然可以重构为长文本但是却不是人类能读懂的语言，故这里需要加另一个约束，即文本必须对人类来说通顺的，考虑增加一个判别器Discriminator，用来对生成的文本的通顺程度打分；<br>总体的模型示意图如下所示：<img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-02.png" class="">
</li>
</ol>
<p>了解了模型的总体结构之后，下面是模型比较详细一点的框架示意图：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-17.png" class=""></p>
<ol>
<li><p>Generator和Reconstructor：都是基于seq2seq结构，原始paper中辅助了attention和copy机制，此处框架示意图没有画出来。这里需要注意的问题是，Reconstructor接收的短文本是由argmax获得的，该函数不可导，故梯度无法传导，按照习惯解法引入强化学习，将Reconstructor端的loss取反，作为reward传导到Generator的短文本生成端作为优化目标。</p>
</li>
<li><p>Discriminator：原始paper提出了两种方法：<br>a. 使用WGAN：假样本是Generator生成的每一个token的概率分布拼接成的矩阵，真样本是真实句子的one-hot向量拼接成的矩阵。使用CNN网络进行编码，使用推土距离(earth mover’s distance)计算一个连续概率分布和一个one-hot分布之间的距离；<br>b. Adversarial REINFORCE：paper中提出的方法。假样本是Generator生成的概率分布进行采样(例如argmax)获得的tokens，真样本是真实句子对应的tokens。使用一个单向的lstm进行编码，每个timestep输入一个token，相应的输出一个分数$s_n$，使用所有N个timestep的分数总和计算$D_{loss}$，如下：</p>
<script type="math/tex; mode=display">
D_2(y^s) = \frac{1}{N} \sum_{n=1}^N s_n
D_{loss} = \frac{1}{K} \sum D_2(y^{s(k)})-\frac{1}{K} \sum _{k=1}^KD_2(y^{real(k)})\\
+\beta _2\frac{1}{K} \sum_{k=1}^K(\Delta _{y^{i(k)}} D_2(y^{i(k)}) -1)^2</script><p>其中$\beta _2$这一项是对梯度做惩罚。可以看到，Discriminator能越早决定句子的真假，则loss会越低。</p>
</li>
</ol>
<h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h3><p>English Gigaword是一个英文摘要常用的数据集，包括3.8百万的doc-summary构成的pair。<br>如下是分别和现有的基于监督学习的工作，自己构建的无监督方法，半监督方法和简单的迁移学习方法的对比。此处的半监督是使用部分的Gigaword数据集的带标签语料，迁移学习是指使用了CNN/Diary数据集的摘要部分的句子作为真样本训练Discriminator。<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-41.png" class=""><br>部分样本结果如下：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-52.png" class=""></p>
<p>其他数据集CNN/Daily和Chinese Gigaword的结果，如下：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-44-05.png" class=""></p>
<h2 id="思考与延伸"><a href="#思考与延伸" class="headerlink" title="思考与延伸"></a>思考与延伸</h2><p>paper提出了一种文本摘要的方法，一个非常大的亮点是无监督，并针对文本生成中常遇到的采样问题(例如argmax)设计了一些解法，解法是目前nlp中常用的对付梯度回传问题的做法，即采用强化学习将梯度信息从不可导的地方，用reward的方式间接回传到前端网络。从实验结果看，paper中提出的Adversarial REINFORCE的做法要比过往的WGAN的做法效果好一点。在summary数据集做评估，效果比不上有监督的方法，这是合理的，但是已经非常惊艳了。</p>
<p>从提供的样例看效果也足够惊人，比较大的问题其实在于文中的预训练，RL在动作空间大的情况下难以收敛，nlp问题对应的动作空间大小常常是词典的大小，几十万级别的动作对RL来说是非常大的数目，一般都会引入预训练步骤去获得一个语言模型用来为RL搜索剪除大量无用搜索路径。但论文使用的预训练模式，应该有泄漏数据的问题，因为Adversarial REINFORCE方法中，辨别器的预训练是拿摘要数据训练的，无形之中辨别器有引导生成器去生成真实摘要数据的趋势，因为辨别器见过真数据，只是原文和摘要之间对不上号，但学习些数据特点还是有的。所以这应该不能算纯粹的无监督学习，这监督信号给的比较间接和隐蔽。</p>
<p>当然，总体来说思路非常之优雅，并且，本文的摘要方法可以不仅仅是对摘要，从大的层面讲，是一种文本转换的方法（此处的转换是长到短），只是从无监督出发的话，长短转换是最直接了当的无监督，需要的约束仅仅在于长度，而长度是非常快捷无成本获得的“监督信息”，所以也就是无监督。如果按照大的层面思考一下，其实文本转换不局限于长度，可以有语言格式的转化，例如翻译，语音识别；或者也可以是图片信息和文字信息的转化，图文互转。其中，翻译的思路其实和微软的对偶学习论文雷同，仅在于微软论文没有涉及GAN，强推对偶学习概念，本文阅读的论文是从编码解码中间态的可读性角度出发，避开了对偶学习的概念，但其实只是论文写作技巧的问题，道理是一致的。图文互转的话，容易令人想起的另一个场景是CycleGAN的图图互转，其实这三者内在的思路都是利用闭环带来新的回馈信息，增加新的约束项。</p>
<p>类似的思路，不同的场景和处理技巧，目前看是CycleGAN认可度最高，除去微软论文复现率太低导致诟病之外，也反映出nlp问题和图像问题的不同。CycleGAN面对图像这种天然存在的物理信号，全程可导，闭环不需要特殊处理，本文阅读的论文和微软的论文面对的是文字的转换，文字信息属于人类抽象的符号，不可导，闭环需要借助RL做特殊处理，目前看训练困难而且个人不严谨地估计一下，因为RL探索的动作空间实在太大，效果的提升可能过分依赖于RL预训练的程度，然后在预训练附近做微调和艰难探索，导致这种方法上限其实不高。</p>
<p>沿着这个思路，还有一篇 <strong>NAACL2019</strong> 的论文（SEQ3）继续进行了新的探索。同样是从文本摘要入手，思路同样是归纳为闭环，但处理文字信号的不可导问题引入了新的思路，gumble-softmax，也是贴着无监督的标签，看起来要比本文的方法更站的住脚。同时，因为舍弃NLP生成问题常用的RL的技巧，直接设计了可导的损失函数，道理上讲效果的上限也会更高一些。具体的阅读和思考，就放在下一篇文章再说了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/" itemprop="url">深度学习中网络参数迭代求解的问题分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T17:01:00+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>接上部分课程讲述深度网络拟合能力和网络深度的关系，本文主要尝试从数值求解的角度理解网络的参数迭代过程。大部分内容来自李宏毅老师关于深度学习理论的公开课，引用的图片来自课程ppt，按照个人理解做了一些梳理和总结。</p>
<p>关于深度网络的参数求解，目前难以有直接求解的方法。我们都是按照梯度下降法来迭代求解。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-05-42.png" class=""><br>但是，我们其实并没有像传统的优化方法一样，苛求优化目标是一个凸函数。实际上，这个参数空间产生的loss函数并不是凸函数，如下：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-05-59.png" class=""><br>从基础常识我们知道，网络的最优解有很多组。例如上图，只要保持黑线和最黑的神经元连接，灰色的线和灰色神经元连接，那么随便颠倒黑色和灰色神经元的位置，这两种网络其实是等价的，即对于相同的input，对应的output都是一样的，但是这是两个不同的参数向量（矩阵），即loss函数起码存在多个最优解。<br>传统的优化理论不认可梯度下降方法来求解非凸问题，因为整个过程不具有数学严谨论证性，尽管目前实践效果极好，这点不是本文重点暂且按下不管。其实到这里仅仅能说明loss函数是非凸的，这个非凸函数有很多个最优解，那假如这个非凸函数没有其他的局部最小值，则还是可以用梯度下降求解这个非凸函数，毕竟只要找到其中一个最优值就行了，如下图：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-07-09.png" class=""></p>
<p>从仅仅寻找其中一个最优解的立场出发，考究该场景的数值求解问题。</p>
<h3 id="优化问题的探究"><a href="#优化问题的探究" class="headerlink" title="优化问题的探究"></a>优化问题的探究</h3><h4 id="1-Hessian"><a href="#1-Hessian" class="headerlink" title="1. Hessian"></a>1. Hessian</h4><p>先不管这个非凸函数是否存在其他局部极小值的问题。先暂时假设没有。那么使用梯度下降法求解的时候依然会遇到其他问题，例如鞍点问题，如下图所示：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-11-10.png" class=""><br>即一个梯度为0的点可能并不是一个极小值或者极大值点，高维情况下这可能是一个鞍点。（低维情况下，例如一维的情况，$f(x)=x^3$中一阶导数为0的点也不是极值点，这是一个拐点。）<br>所以如何衡量一个一阶导数为0的点是鞍点还是极值点，类比于$f(x)=x^3$中使用二阶导来判断是否拐点的情况，这里也需要使用二阶导，不过高维的二阶导是一个矩阵的形式，称为海塞矩阵(hessian matrix)。<br>首先引入一点关于海塞矩阵：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-11-36.png" class=""><br>按照参数是一维的情况画出示意图，如下：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-12-06.png" class=""><br>一维情境下：$f(x) \approx f(x_0)+f’(x_0)(x-x_0)+\frac{1}{2!} f’’(x_0)(x-x_0)^2$<br>故当$f’’(x_0)<0$时，一阶近似函数由于少了一个负数项，故其图像在原函数之上。反之则一阶近似函数的图像在原函数图像之下。
至此，我们有两个选择：a)做的美一点，即直接将海塞矩阵融合到梯度下降法上面，其实就是办成一个二阶优化的做法；b)简单一点，使用梯度下降法求解从极值点之后再用海塞矩阵来判断极值点是否是鞍点；
a). 
<img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-12-35.png" class=""><br>然而，其实仔细想想又会发现，二阶优化在求的可能是一个局部极大值，也可能是一个鞍点。因为二阶优化的过程其实是在不断地对点x0周围的二阶拟合函数求取极值点而已，并不关心该极值点是个极大还是极小，也不关心是否是拐点。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-13-50.png" class=""><br>b).<br>矩阵H正定：对所有的非零向量$x$，$x^T H x&gt;0$成立。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-14-11.png" class=""><br>这张图中有一点错漏，$xHx \gt 0$应该是$x^T Hx \gt 0$。另一种情况，如果$x^T H x \ge 0$的时候，无法判断是否是极小值。因为这里只写出了二阶近似，等于0的时候，三阶以上余项的正负决定了该处是极小还是极大值。<br> 另外也可以从特征值分解的角度来看这个问题。<br>我们知道，海塞矩阵H是一个对称实矩阵，此时矩阵H正定可以推导出：矩阵H的特征值都是正的，并且所有的特征向量都彼此正交。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-14-30.png" class=""></p>
<p>如上图，每一个向量$x$都可以分解到特征向量构成的正交基上，计算出来的结果可以看出，任意一个$x^T H x$都是正的，某个梯度为0的点如果海塞矩阵是正定的，则往四周走都是增加的，故这是一个local minima。<br>不过，当前对于梯度下降法在深度网络中到底寻找到的点是什么点还是有点存疑，例如下面这几张图：<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-14-53.png" class=""><br>这个图说明了训练某个网络收敛之后，将梯度的长度打印出来，发现其实并不是0.<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-15-05.png" class=""><br>这个图说明了收敛的过程中，并不是梯度不断减小的过程，这一点是违背一般直觉的。不过立足已有事实，尝试去解释事实，这个图其实也可以找到解释，梯度陡然增加的时候可能就是在逃离鞍点的时候。<br>补充：<br>我们知道hessian是二阶导，有时候，一些局部点的周围很‘平’，导致了这个局部点周围的二阶导都是0，即hessian是一个0矩阵。如下图，此时根据hessian也是难以判断的。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-16-08.png" class=""><br>这种时候，就无法使用hessian来判断到底是不是鞍点，考虑使用更高阶的导数或者其他方法。</p>
<h4 id="2-is-local-minima"><a href="#2-is-local-minima" class="headerlink" title="2. is local minima?"></a>2. is local minima?</h4><p>前面还留了一个问题：loss关于参数的函数是非凸函数，这个函数是否存在其他局部极小值的。<br>在16年前后，有工作证明了，不加非线性函数的情况下，深度网络满足一点前提条件（hidden_dim大于等于input_dim和output_dim）的时候，即可实现local minima等于global minima。<br><img src="/2020/02/06/hung-yi-Lee%EF%BC%9ADeepLearningTheory2-md/2020-02-06-17-17-33.png" class=""><br>不过，不加非线性激活的网络其实没什么实际用途，现在的审稿也逐渐不收这类paper了。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文尝试从优化问题出发，在理论层面去阐释深度网络数值求解会遇到的一些问题。但实际上由于无法用现有的数学工具完备地描述网络的工作原理，因此目前的数值求解方式只能先从实践层面获得认可，难以内化出统一严谨的数学理论。这一点也是深度学习学术界和工业界观点上互相看不上的关键，像极曾经的理论物理和数学之间的对立，物理讲求从实际建模，经过猜想和验证，能自洽甚至不够自洽，但只要能解决大部分场景下的实际问题，理论即为合理。数学似乎站在一个更高的层次，看不上物理派不够严谨的猜想，期待找到能抽象并解释物理定律的工具，但这并不妨碍两者各自沿着自己的发展轨迹前进。同理，深度学习极强的拟合能力和泛化性能造就了其在实践中攻城略地，吸引了更多的从业者，从实践出发也好，尝试从理论着手也好，都引入了更多的发展可能，共同为初级智能时代建设贡献着力量。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/" itemprop="url">深度学习中网络拟合能力和网络深度的理论分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-05T16:52:11+08:00">
                2020-02-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文大部分内容来自李宏毅老师关于深度学习理论的公开课，引用的图片来自课程ppt，按照个人理解做了一些梳理和总结。</p>
<h3 id="1-can-shallow-NN-fit-any-function"><a href="#1-can-shallow-NN-fit-any-function" class="headerlink" title="1. can shallow NN fit any function?"></a>1. can shallow NN fit any function?</h3><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-38-41.png" class="">
<p>已经知道的结论是，一个单隐层fcnn而已拟合任意连续函数，那么如何证明呢？<br>首先，在该问题场景下定义出拟合的概念，如下：<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-39-02.png" class=""><br>假定输入的x∈[0,1]，输出y不限定，同时定义一个映射空间和k有关，则对于任意小的数$\epsilon$ ，[0,1]区间之间拟合误差的最大值需要小于该设定的$\epsilon$，因为只要误差最大值小于设定的数，即可推出L2积分误差小于该设定值，符合函数拟合的定义。</p>
<p>证明：<br>这里只证明待拟合函数$f$是L-Lipschitz函数的情况。定义如下：<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-40-20.png" class=""><br>意味着说输出的变化会小于输入的变化，即该函数起伏较小，是一个较平滑且起伏缓和的函数。</p>
<p>然后回到拟合的问题上，对于待拟合函数f，如下图，假如我们在f上均匀采样若干个点，每两个点的距离设定为$l$，然后将这些点连接起来，就可以得到初步的拟合函数g，同时，根据拟合的定义，每一段的最大拟合误差需要小于设定的$\epsilon$，又根据Lipschitz条件，我们可以知道图中的$error$一定是小于等于$ l L $，因此只要使得$ l L &lt;= \epsilon $成立，则目标达成，由此可以得到$ k=\frac{1}{l} &gt;= \frac{L}{\epsilon} $<br>即只要切分的数量k够多，对于任意小的$\epsilon$都可以满足拟合条件，由此证明完毕。<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-23-24-56.png" class=""><br>直观地来说，上图的折线图在实际的网络中可以这样凑出来，如下图：<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-41-10.png" class=""><br>每个小区间的直线可以使用两个ReLU函数拼接出来，故最终的网络如下，可以先使用两倍的神经元和ReLU数量得到每段折线图，然后将这些折线图加起来总的折线图。下图的NN实质上是单隐层。<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-41-29.png" class=""></p>
<p>由此可以证明，单隐层网络足够拟合任意的连续函数。但是客观上需要的神经单元可能不需要用到以上证明用的这么多。</p>
<h3 id="2-So-why-deep"><a href="#2-So-why-deep" class="headerlink" title="2. So why deep?"></a>2. So why deep?</h3><p>既然上一节已经证明了shallow NN已经足够拟合任意连续函数，为什么还这么热衷于deep NN?<br>受限于严谨的数学描述工具来描述深度网络，其实很难完美地回答这个问题。<br>一些感性的答案，基本是从宏观的，笼统的角度来阐述。</p>
<ol>
<li>从电子设计中的逻辑电路设计这个角度来类比。我们知道，使用一个两层的基础逻辑器件可以搭建出任何的单元逻辑门。同样的，两层的网络可以拟合任何的连续函数，不严谨的说，这两者具有类比效果。我们知道，如果设计一个逻辑电路，设计深度较深的，可以节省逻辑门。从这里类比到，同样的神经元数量，搭建深度较深且窄的网络，拟合效果强于浅且宽的网络。<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-41-59.png" class=""></li>
<li>同样的神经元数量，更深的网络可以更容易地拟合复杂函数。先看另一个问题。一个具有$n$个神经元的函数，能产生的等价函数是一个多少段的分段函数？答案是很难说清，但是可以证明，上界是$2^n$，示意图如下所示：每个神经元有两种激活状态，故总体来看，网络一共是$2^n$种激活可能。（这是一个上界，实际上可能是很难达到这个上界的）<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-42-35.png" class="">
针对一个具体的例子，例如$f(x)=x^2$，需要多少个片段才能拟合地好，这个比较难计算，但是如果规定分段函数的每一段都是等宽的，则比较容易算，结果如下图：<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-42-47.png" class="">
附：实际上这种平方的网络是用很多普适性的。如下：<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-42-57.png" class="">
可以非常简单地获得两个数的乘积网络。然后从这点出发，可以继续产生多项式的网络，然后再按照泰勒展开式进一步产生指数型的网络，如下：<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-43-07.png" class="">
说回我们的问题，其实到这里还是证明不了deep的下限，但是，已经能找到deep的其中一个范围，即$O(log_2 \frac{1}{\sqrt{\epsilon}})$。同时，用其他的复杂方法，可以证明shadow的方法的上限是$O(\frac{1}{\sqrt{\epsilon}})$，故deep一定好过shadow（当然，有一个前提，即要拟合的函数要比$y=x^2$更复杂。形如一次函数，shadow的可能更好）。<img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-43-21.png" class="">
</li>
</ol>
<p>以上是对于网络深度和拟合能力之间的关系的一些探究，还留下一个关于deep下限的问题，这实际上还没有较一统的结论，目前有一些尝试求证deep的下限的工作：<br><img src="/2020/02/05/hungYiLee-DeepLearningTheory1-shadowOrdeep-v1/2020-02-05-22-44-23.png" class=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">MioKinHuang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MioKinHuang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
