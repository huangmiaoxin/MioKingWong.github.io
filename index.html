<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="MioKinHuang &#39;s Blog">
<meta property="og:url" content="https://huangmiaoxin.github.io/index.html">
<meta property="og:site_name" content="MioKinHuang &#39;s Blog">
<meta property="article:author" content="MioKinHuang">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://huangmiaoxin.github.io/"/>





  <title>MioKinHuang 's Blog</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MioKinHuang 's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/" itemprop="url">机器学习当前发展中的一些问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T23:18:24+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是李宏毅老师的公开课的学习记录，全部图片来自李老师的课件。文字按照个人理解做了组织和阐释。主要探讨了当前机器（深度）学习发展中的一些大的技术问题。</p>
<h3 id="1-可解释性"><a href="#1-可解释性" class="headerlink" title="1. 可解释性"></a>1. 可解释性</h3><h4 id="1-1-Local-explanation"><a href="#1-1-Local-explanation" class="headerlink" title="1.1 Local explanation"></a>1.1 Local explanation</h4><p>局部解释：例如：网络将某个图片分类为猫，希望知道网络通过哪个区域判断该图片。<br>直观思路：可以使用一个小方块随机遮挡图片中的某块区域，看分类网络把遮挡之后的图像分类为猫的概率的变化，判断该区域对判断为猫的重要程度。如下：蓝色的区域表示对分类结果影响较大的区域。<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-01.png" class=""></p>
<h4 id="1-2-Global-explanation"><a href="#1-2-Global-explanation" class="headerlink" title="1.2 Global explanation"></a>1.2 Global explanation</h4><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-15.png" class="">
<p>直观思路：随机生成一些图片，直接输入到分类网络，筛选出网络认为的某类图片，例如最后的分类向量第一维代表数字0，则筛选出使得该维度强度最大的输入图片，标记为图片认为的数字0.但实际上，这样获得的图片，完全不是人看起来的0，如上图。<br>全局解释：例如：网络将某个物品分类为猫，希望解释为什么网络认为是猫。<br>原因应该是，高维空间非常广阔，分类网络被训练的时候只是简单拟合了自然图像，这只是图像空间中非常少数的一部分，当在图像空间中随机采样获得‘图片’时，存在众多图像可以使得分类网络误判。可以尝试做一下限制，例如，正常我们理解的数字0的图像，其实是少数黑色像素构成数字0，图像背景白色，因此可以考虑加一些这类约束，如下图：<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-24.png" class=""><br>增加了这种约束之后，筛选的图像质量明显提高，尤其是数字6，基本看得到数字的轮廓。<br>上面这样的约束过于简单，输入的图像实际上很多时候依然不是正常的人类认为的图像，可以考虑进一步增加约束，使得输入的图像是真实图像，可以考虑生成模型，如下：<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-32.png" class=""><br>使用GAN或者VAE的生成器，通过随机采样潜在向量z，获得真实图像，然后输入到分类网络做分类并筛选，基本上这种做法效果会好很多。<br>下面是《Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space》中的结果，效果非常惊艳。<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-43.png" class=""></p>
<h4 id="1-3-using-a-model-to-explain-another"><a href="#1-3-using-a-model-to-explain-another" class="headerlink" title="1.3 using a model to explain another"></a>1.3 using a model to explain another</h4><p>可以尝试使用可解释性的模型来解释黑盒网络，例如，使用容易解释的线性模型来解释某个区域内的黑盒网络。如下：<br>通过在某个点的附近随机采样多一些点，然后使用线性模型来拟合这些点，然后分析线性模型的参数来判断黑盒在该点附近的一些性质。<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-24-52.png" class=""><br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-25-00.png" class=""></p>
<p>具体的例子结合paper《LIME - Local Interpretable Model-Agnostic Explanation》，该paper提出了一种使用可解释模型来解释黑盒网络的local explanation的方法LIME。<br>思路是直接对某个样本制作多一些测试数据，用来测试黑盒子在该样本点上的一些性质，例如下图通过随机遮挡图像的某些区块创造新的测试数据，然后将所有数据输入到网络中，记录下输出结果，然后使用线性模型来拟合输出结果。但实际上很难直接用线性网络来拟合图像分类结果，考虑到测试数据其实都是来自同一个图像，因此不需要再靠网络来编码众多区块，直接按照顺序把区块编号即可，将区块的有无作为编码信息，把01编码串作为线性模型的输入。最后，可以按照训练好的线性模型在每一个维度的权重来判断模型认为每个区块对判断为青蛙（图中分类结果）的重要程度。<br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-25-22.png" class=""><br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-25-31.png" class=""><br><img src="/2020/02/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5/2020-02-06-23-25-43.png" class=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/" itemprop="url">无监督带条件约束的GAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T23:09:58+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是李宏毅老师的公开课的学习记录，全部图片来自李老师的课件。文字按照个人理解做了组织和阐释。主要介绍了基于GAN衍生出来的一些技术，无监督的带条件约束GAN，典型如CycleGAN。</p>
<hr>
<h3 id="Unsupervised-Conditional-Generation"><a href="#Unsupervised-Conditional-Generation" class="headerlink" title="Unsupervised Conditional Generation"></a>Unsupervised Conditional Generation</h3><p>在conditional GAN中，例如训练一个给定文字描述产生对应的图片的模型，需要收集一些文字和对应图片的平行语料。但某些情况下，conditional GAN可以不用平行语料。一个常见的例子是图像风格的转化，如下：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-02.png" class=""><br>将写实风格的图片直接转化为梵高风格的图片。两个思路，一个是生成方法：使用根据两个domain之间的样本存在轮廓相同等特征，加一些约束，构建conditional GAN。另一个是比较直观的思路：将风景画和梵高的画都分别编码到一个空间，然后利用一些对齐的方法（例如使用少部分的平行数据）将这两个空间对齐。</p>
<h4 id="1-direct-transform"><a href="#1-direct-transform" class="headerlink" title="1. direct transform"></a>1. direct transform</h4><p>把domain1的数据直接作为条件输入生成器G，产生新数据，然后使用分类器D进行判断，如下：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-11.png" class=""><br>这样直接做下去，生成器G可以直接生成和输入完全无关的图像，例如直接输出梵高的自画像，就能直接骗过分类器D。所以需要增加额外的约束，一个很明显的约束是限制生成器的输出和G在轮廓上相近，如下图：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-19.png" class=""><br>直接使用一个预训练的CNN接收G的输入和输出，约束这两个输出尽可能相近，即保证了G的生成图像轮廓和输入图像轮廓相近。<br>另一个约束可以是按重构误差的思路来，如下图：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-26.png" class=""><br>写实风格转梵高风格的对偶模型是梵高风格转写实风格，引入这个对偶模型，对两种图像分别进行重构。这是CycleGAN的做法，但实际上，这种重构不一定有用，因为很明显地两个模型可以合作唱双簧，中间产生的图像不一定是正常的图片。但实际上，这样做出来的效果一般还不错，原因可能是对模型来说，这种映射能力不是无限大的，为了能够重构，中间产生的图像必须轮廓上和输入图像较为接近。</p>
<p>实际上，有另一个更为惊奇的做法是，不加约束，不管生成器G会直接生成和输入完全无关的图像去骗过分类器D这件事。这在网络深度较浅的时候也可以出效果，原因应该是模型倾向于生成轮廓上变动不大的图片。</p>
<h4 id="2-projection-to-common-space"><a href="#2-projection-to-common-space" class="headerlink" title="2. projection to common space"></a>2. projection to common space</h4><p>这种想法比第一种想法更直观，直接把两种风格的图像各自完成autoencoder，如下：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-39.png" class=""><br>此时中间获得的图像表示向量v可以看做携带了图像的绝大部分信息，但是，这两个autoencoder获得的v意义是不一样的，例如假设v有3维，对写实风格的图像来说v的三个维度分别表示头发的长度，眼睛大小，脸的大小。但是对于漫画风格的图像来说v的三个维度可能是代表眼睛大小，脸的大小，头发的长度，甚至于说这种差别可以完全不是顺序问题。因此，这两个图像表示空间可能完全是无法对齐的。<br>解决方法：</p>
<h5 id="2-1-共用参数"><a href="#2-1-共用参数" class="headerlink" title="2.1 共用参数"></a>2.1 共用参数</h5><p>这个问题的一个解决方法是，共用两个autoencoder的参数，尽可能让这两个表示空间v相同，但实际上又不好使用完全相同参数的autoencoder，因此仅仅在encoder的最后几层和decoder的前几层共享参数。或者想要直接整个autoencoder共享参数的话需要在encode的时候顺便输入一个flag，表示此时是编码写实风格的还是漫画风格的图像，然后对应解码，测试的时候只需要将flag颠倒，即可得到风格转换的目标图像。<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-12-53.png" class=""></p>
<h5 id="2-2-强迫空间对齐"><a href="#2-2-强迫空间对齐" class="headerlink" title="2.2 强迫空间对齐"></a>2.2 强迫空间对齐</h5><p>另一个解决方案是增加一个分类器判断两种autoencoder的中间表示v来自哪个domain。该思路的出发点是既然两个空间是不对齐的，那么可以使用分类器识别出来，然后按照以分类器识别不出来为目标去调整两个autoencoder使得空间对齐。</p>
<p>题外：<br>有另一个问题是，使用均方差作为重构误差会带来生成的图像较模糊这个问题，很直觉地，按照GAN的思路，该问题可以使用引入分类器D来缓解，如下，其实就是VAE-GAN的思路。<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-13-02.png" class=""></p>
<h5 id="2-3-Cycle-consistency"><a href="#2-3-Cycle-consistency" class="headerlink" title="2.3 Cycle consistency"></a>2.3 Cycle consistency</h5><p>另一个解决方法依然是利用重构误差，如下：<br><img src="/2020/02/06/%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E5%B8%A6%E6%9D%A1%E4%BB%B6GAN/2020-02-06-23-13-13.png" class=""><br>构建的回环是从写实风格的图片1到漫画风格，然后继续将漫画风格转换成写实风格的图片2，图片1和2构成重构约束。其实这整个过程和CycleGAN的做法一直，只不过这里只用了一个重构过程（其实也可以把漫画风格的进行重构）。在CycleGAN中没有显式地画出encoder和decoder的过程，这里的ENx和DEy就构成了CycleGAN中的一个生成函数$G_{x-&gt;y}$，ENy和DEx就构成了CycleGAN中的一个另一个生成函数$G_{y-&gt;x}$。以上就是ComboGAN，和CycleGAN其实是同个东西。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/" itemprop="url">各式各样的GAN——WGAN，EBGAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T23:02:44+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是李宏毅老师的公开课的学习记录，全部图片来自李老师的课件。文字按照个人理解做了组织和阐释。主要介绍了几种GAN——WGAN，EBGAN</p>
<h3 id="1-WGAN"><a href="#1-WGAN" class="headerlink" title="1. WGAN"></a>1. WGAN</h3><h4 id="1-1-JS-divergence-is-not-suitable"><a href="#1-1-JS-divergence-is-not-suitable" class="headerlink" title="1.1 JS divergence is not suitable"></a>1.1 JS divergence is not suitable</h4><p>关于这个问题其实讨论地很多，一个目前被广泛接收的观点是在原始GAN中使用的JS散度的确不够合适，问题在于JS散度在两个分布数据没有重叠时JS散度值是一个固定的数值，如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-04-26.png" class=""><br>不管生成数据已经有多接近真实分布，只要没有重叠，JS散度都不会有反应。因此当生成数据在相比之前更接近真实数据了，但是JS散度依然没有给更好的数值，这显然对模型参数的引导作用不够。这个问题其实可以直观地感受到，因为：我们使用最优分类器来判断两个分布的接近程度，当两个分布不重叠时，都可以容易地区分开，因此这种情况下分布的接近程度是一样的，都是不接近。</p>
<p>关于这个问题，较普遍的两种解释如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-04-36.png" class=""><br>第一种解释认为，一个M*N大小的图像其实不是MN维的数据，而是MN维空间中的一个低维manifold，大体意思是图像数据只需要很小的维度就能直接描述。在MN维空间中，生成数据和真实数据的真实维度很低，因此可能很难有重叠。例如图中的两个直线，重叠部分是两个点，几乎可以忽略不计。<br>第二种解释认为，就算两个数据分布有部分重叠，但是平常我们采样的数据毕竟不是完全充分的，因此就算有极少的重叠，对于最优分类器来说，依然可以视为毫无重叠。如图中可以直接找到一个分类平面将两种采样数据分隔开，虽然对于理想分布来说这两种数据其实已经有少数的数据点重叠了。</p>
<h4 id="1-2-Least-Square-GAN"><a href="#1-2-Least-Square-GAN" class="headerlink" title="1.2 Least Square GAN"></a>1.2 Least Square GAN</h4><p>原始的GAN另一个问题是，由于使用了sigmoid函数作为分类的激活函数，生成数据可能较难朝着真实数据的方向趋近。因为sigmoid函数在两端的斜率较小，导致梯度计算的结果很小。一个可能的解决方案是不使用sigmoid函数，直接使用线性的激活函数。<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-04-52.png" class=""></p>
<h4 id="1-3-Wasserstein-GAN"><a href="#1-3-Wasserstein-GAN" class="headerlink" title="1.3 Wasserstein GAN"></a>1.3 Wasserstein GAN</h4><p>上面说明了，原始的GAN使用的JS散度不利于模型收敛。本质在于JS散度衡量两个分布的距离的时候不够合理。改进的思路可以是换用其他一些较好的距离衡量方式像Wasserstein距离，该距离计算方式可以在两个分布不重叠时也能反映分布的距离。<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-05-03.png" class=""><br>Wasserstein距离通俗上也称为推土距离。直观上就是把一堆土的形状修整成另一堆土的形状时所需要搬动的最少的土量。如何解这个最优的搬土量也是一个重点。这种距离计算方式有好处：两个分布之间的接近程度应该是渐变的，推土距离也是渐变的（JS散度是突变的），因此推土距离用来描述两个分布之间的接近程度更为合理。</p>
<p>插话：<br>关于这种渐变的距离描述方式为什么好其实很容易理解，不需要举例。不过在课程中李宏毅老师给了一个很有意思的例子，也放过来。如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-05-16.png" class=""><br>生物进化出眼睛的过程是渐进的，通过多种微小的累积最终产生了眼睛，并非突然产生。首先在皮肤上突变产生了一些能感光的细胞，后来又产生了凹陷的形状，由于凹陷的个体能接受各个方向的光线并判断光源，可能在后续的一些环境竞争中被选择出来，属于有利突变。再后来，由于凹陷容易聚集灰尘，因此突变产生睫毛和包裹外皮的性状又被选择了出来，最终一步步进化成了现在看到的器官眼睛。在这个过程中，需要有多种突变，并且需要环境有一套筛选标准以知道哪种突变是有利的，鼓励它被保留下来。类比到GAN中，原始的JS散度在两个分布没有重叠的时候给出的判断值不变，因此在还没有重叠的时候尽管生成分布更加靠近真实分布了但分类器根本无法判别，最终导致收敛困难。</p>
<p>有了Wasserstein距离，可以考虑把这个度量方式应用到GAN中，实际的推导过程很复杂，最终的结果如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-05-28.png" class=""><br>公式如下：</p>
<script type="math/tex; mode=display">
V(G, D) = \mathop{\max}_{D \in 1-Lopschitz}\{E_{x \sim P_{data}}[D(x)] - E_{x \sim P_G}[D(x)]\}</script><p>公式表明了分类器D的优化目标是使得生成数据在分类器中得分最低，真实数据在分类器中得分最高，这本来没什么问题，但公式有约束条件，即分类器D的函数必须满足一阶的利普希茨条件。该条件的定义如上图，可能可以利用中值定理，转换成该函数在区间$[x_1, x_2]$上导数值绝对值小于等于$K(K\text{此时等于}1)$，很明显意思就是函数在该区间内需要尽可能“平滑”，不能太陡峭。<br>现在的问题转化成如何在带一阶利普希茨条件这样一个全局约束的情况下找到优化器D的参数。这其实不好做，因为对梯度下降法来说难以加入这种约束。在原始的WGAN论文中其实也没有解这个问题，原始的论文直接将这个条件弱化成梯度裁剪，希望通过把模型参数$w$约束在$[-c, c]$中，迫使模型代表的函数足够平滑。这显然不是等价处理，但从提出Wasserstein距离来解决原始GAN训练的问题并且给出了初级的解法这一点来看，这个工作的贡献依然非常大。</p>
<h4 id="1-4-improved-WGAN"><a href="#1-4-improved-WGAN" class="headerlink" title="1.4 improved WGAN"></a>1.4 improved WGAN</h4><p>后续有paper提出了别的解法，如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-06-22.png" class=""><br>一阶利普希茨条件数学上完全等价于在区间$[x_1, x_2]$上所有的导数值绝对值小于等于$K(K\text{此时等于}1)$，具体到该问题场景，此处的有效区间是所有数据样本，因此问题转化成对于所有的数据样本，分类函数D的梯度值的大小都小于等于1，因此可以按照这个转换来设计正则项，如图中所示，使用分类函数D梯度值超过1的部分来量化地代表数据x不符合一阶利普希茨条件的程度，对所有不符合一阶利普希茨条件的数据x加和获得惩罚项。但实际上难以采样获得所有的数据样本x，因此只能将该条件稍微弱化一下，转化为在某个划定的数据集合$P_{penalty}$上计算该惩罚项，同样如上图所示。</p>
<p>关于数据集合$P_{penalty}$的获取，论文中给出的思路如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-06-37.png" class=""><br>集合$P_{penalty}$的数据点是在生成数据点和真实数据点两个采样点的连线中采样获得。所以集合$P_{penalty}$是在真实分布和生成分布之间，这是较为符合直觉的，$P_{G}$在朝着$P_{data}$趋近的路上的这些数据点必须符合约束条件，其余空间中的点不需要考虑因为分类函数D并不会接收这些点。</p>
<h4 id="1-5-algorithm-pseudo-code"><a href="#1-5-algorithm-pseudo-code" class="headerlink" title="1.5 algorithm pseudo code"></a>1.5 algorithm pseudo code</h4><p>原始的GAN和WGAN的代码流程对比如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-06-50.png" class=""><br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-06-59.png" class=""><br>从代码层面来看，其实改动很小。</p>
<h3 id="2-EB-GAN-energy-based-GAN"><a href="#2-EB-GAN-energy-based-GAN" class="headerlink" title="2. EB GAN(energy-based GAN)"></a>2. EB GAN(energy-based GAN)</h3><p>在一般的GAN中，获得最优分类器需要通过训练得到，并且最优分类器需要在生成器的进化中跟着不断进化。在启动阶段，生成器非常弱，产生的图像都是噪声，这种情况下很容易获得一个最优分类器，但其实这时候的分类器很弱。又在这个很弱的分类器D的基础上去提升生成器其实也提升不多。这是GAN训练困难的一个原因。突破这个困境的一个思路是有其实阶段G或者D有较好的能力。</p>
<p>EB-GAN为分类器D设计了特别的机制使得分类器可以被简单地预训练，因此起步阶段分类器D一般已经有了较好的能力，可以加快后续的训练过程。EB-GAN的分类器设计如下：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-07-11.png" class=""><br>EB-GAN的分类器基于autoencoder，将生成图片使用autoencoder进行重构，计算重构误差，使用重构误差的相反数最为判断真假的打分。这种做法基于一个假设，即图片如果是真实的，那么autoencoder应该可以重构地较好，反之不能。这个假设其实也有一定的道理。在这里，autoencoder可以被较好地预训，因此分类器D一开始就有了较好的能力，有助于后续的训练。</p>
<p>EB-GAN中分类器D的打分是重构误差的相反数，因此上界是0，下界是无穷大。如果直接地最小化生成样本的分数，最大化真实样本的分数，则两类loss的数量级相差太多，模型完全可以不管真样本的loss，直接输出一些噪声，此时假样本的loss会很小，假样本loss减小的幅度超过真样本增加的幅度即可，模型完全失去能力。在EB-GAN中，对于假样本需要限制loss的数量级，方法是使用margin loss，即假样本的得分只要小于某个设定的数值m即可，不需要最小化成无穷小。如下图：<br><img src="/2020/02/06/%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84gan-wgan-and-ebgan/2020-02-06-23-07-23.png" class=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" itemprop="url">生成对抗网络（GAN）和Conditional GAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T22:44:02+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是李宏毅老师的公开课的学习记录，本节主要介绍的是生成对抗网络（GAN）和带条件的GAN。笔记的全部图片来自李老师的课件。文字按照个人理解做了组织和阐释。</p>
<h3 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1. introduction"></a>1. introduction</h3><p>生成器+判别器（Generator+Discriminator）<br>Generator<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-46-44.png" class=""><br>原始paper中使用深度网络作为生成器，输入一个随机向量z，输出图像。</p>
<p>Discriminator<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-46-56.png" class=""><br>判别器依然使用深度网络学习一个函数映射关系，输入一个图片，输出一个标量，表示这个图像接近真实图像的程度。</p>
<img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-47-07.png" class="">
<p>伪代码比较易懂，将G和D交替更新。例如，先固定G，训练D使得其分辨真实样本和生成样本，然后固定D，训练G使得其输出的样本在D上获得高分。</p>
<p>实际上可以要获得这样的生成器，一个更容易的，直观的想法是直接使用auto-encoder，训练模型完成图像的重构，即可获得图像的中间向量表示和一个decoder，作为G。但实际上，这样获得的向量表示空间非常不连续，如下图：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-47-18.png" class=""><br>训练好的decoder在左倾斜的数字1和右倾斜的数字1之间的空间是未训练过的，因此无法在这两个表示向量之间进行插值去获得正的数字1，所以这个decoder其实并不好。一个很容易想到的改进是使用VAE，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-47-27.png" class=""><br>使用VAE获得的decoder效果要好很多。</p>
<p>VAE做出来的decoder和GAN获得的G相比：GAN可以视为是结构化学习的方法。使用G产生样本，然后按照D的判断决定样本的质量。而在VAE中，算是使用G产生样本，然后计算样本的重构误差，误差一般使用像素的均方差，但实际上图像的质量并不是由均方差来决定。因为无法将误差理想化地训练为0，所以最终网络必须进行妥协，把误差分摊到多个像素上，但人类实际上不关心每个像素的质量，更关注画面总体的质量，例如产生人脸时，局部的部分如眼睛鼻子等的边缘不能模糊，人脸总体必须分布协调等。因此使用均方差并不合适。但是采用何种误差方法很难言明，干脆直接使用网络来决定，该网络称为D，把生成样本和真实样本都作为判别器的输入，假样本判断为0，真实样本判断为1。 从结构化学习的角度来看这个问题，如下图：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-47-41.png" class=""><br>均方差的损失函数产生的结果会顾及组成图像的某些部件，例如图中数字2的不同部位（上方的弯曲，下方的横线等部件），但是部件之间的组合没有十分合理，例如左边的的数字2，右下方有一个像素游离于数字2这个主体之外，实际上，对于普通的生成器和像素均方差损失函数，很容易出现这种问题，因为生成器生成不同像素的时候没有大局视野，生成不同像素的神经元之间没有配合好。而对于GAN来说，引入了D，输入的是一整个生成样本，优化这个画面数据使其接近真实图像，可以使得D具有大局观，产生结构搭配上更合理的图像。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-47-50.png" class=""></p>
<p>G在生成数字2的各个部件时，可能很难考虑多个部件之间的结构关系，但是当生成了完整的数字2之后，D只需要检查一下结构关系是否合理，这是一件较直观的事情。</p>
<p>从结果上来看，VAE产生的效果确实相较于GAN更差一些。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-01.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-09.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-16.png" class=""><br>对比人脸，使用VAE获得的人脸在边缘细节上较为模糊，相比之下GAN更具有全局观。但是VAE也有自身的优点，从第一个统计图可以看出，VAE产生的样本数据质量比较均匀（虽然模糊），GAN产生的数据质量差距较大，但最好的数据的质量明显高过VAE。从数据点分布来看，第一个图的蓝色点是真实样本点，红色是GAN产生的样本点，基本逼近了真实样本点的分布情况。第二个图的绿色点是真实样本点，蓝色是VAE产生的样本点，总体效果还可以，但是不够精准。</p>
<p>思考：<br>假设可以解下面的方程：</p>
<script type="math/tex; mode=display">
x = \text{argmax}_{x} (D(x))</script><p>那么是否能不使用生成器，直接用真实样本来训练一个判别器。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-28.png" class=""><br>对于一个训练到一半的D，可以随机采样一些数据，看D的打分，记录分数高的图像，视为假样本，继续训练D，重复以上过程。<br>但实际上，因为高维空间非常广阔，要解argmax方程极度困难。<br>从这个角度来看，生成器其实在尝试做argmax的工作，如下图：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-38.png" class=""></p>
<p>总结：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-48-48.png" class=""><br>GAN使用深度网络来代替以往的人工定义的损失函数，在图像问题上摒弃了像素均方差的方法，可以获得更加真实的图像。从结构化学习的角度看，原先的均方差方式没有办法考虑成图的组件之间的结构联系，不符合真实图像的规律，而D可以直接输入整个图像，发掘图像与真实图像的差别，进而调节生成器G。</p>
<p>从拟合数据分布的角度来看，GAN的过程直观展示如下图：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-49-20.png" class=""><br>为了直观，将图像数据视为1维，绿色部分是真实数据分布。<br>一开始生成器G产生的数据分布和真实分布并不一致。通过采样G的数据和真实样本数据，使用D的判断，告诉G这部分生成数据的概率应该增大或者减小，从而改变G产生的数据的分布。当最终达到平衡时，G产生的数据分布逼近了真实分布。</p>
<h3 id="2-推导"><a href="#2-推导" class="headerlink" title="2. 推导"></a>2. 推导</h3><h4 id="极大似然和KL散度"><a href="#极大似然和KL散度" class="headerlink" title="极大似然和KL散度"></a>极大似然和KL散度</h4><p>首先回顾生成模型在做的事情：寻找数据的分布。详细一点来说，如下图：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-49-35.png" class=""><br>按照已知的$P_{data}(x)$，采样获得$x_1, x_2, …, x_m$，构成数据集X。要拟合真实分布$P_{data}$，等价于拟合数据集X。首先预定某一个分布，参数为$\theta$，从中采样数据$P_G(x;\theta)$，按照极大似然的思想，假如参数$\theta$的分布拟合了真实数据$P_{data}(x)$，则从参数$\theta$ 的分布中采样m个数据等于数据集X的概率应该最大，据此获得目标函数：</p>
<script type="math/tex; mode=display">
\prod_{i=1}^m P_G(x_i, \theta)</script><p>上述公式总体上看是在求取真实分布$P_{data}$和生成分布$P_G$的差距，具体的计算细节是化为求真实数据$x_i$和生成数据$x \sim P_G(x;\theta)$的交叉熵。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-50-05.png" class=""><br>而实际上，由于该过程中$P_{data}$和参数$\theta$无关，因此可以将$P_{data}$的信息熵加到交叉熵中，构成相对熵(KL散度)。<br>综上，生成模型的损失函数实际上是在求取生成分布和真实分布之间的KL散度。</p>
<h4 id="GAN中衡量两个分布的差距"><a href="#GAN中衡量两个分布的差距" class="headerlink" title="GAN中衡量两个分布的差距"></a>GAN中衡量两个分布的差距</h4><p>公式\ref{mle_gen}中的参数为$\theta$的分布可能非常复杂，难以定义。但实际上一个复杂分布可以使用一个标准正态分布后接一个深度网络来近似，此处的深度网络能把标准正态分布采样获得的向量任意变换成为一个复杂分布采样获得的向量，实际上是把标准正态分布“扭曲”成为任意分布。此时上述目标函数中的参数$\theta$实际上包含了标准正态分布的参数和深度神经网络的参数，不过标准正态分布的参数已知因此参数$\theta$实际上是深度网络的参数。如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-50-14.png" class=""></p>
<p>此处，GAN不是直接计算$P_G$和$P_{data}$之间的某种接近程度（散度）。而是直接利用一个深度网络来判断这种“差距”，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-50-22.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-50-29.png" class=""></p>
<p>直观上来说，我们希望生成分布和真实分布非常接近，但是现在暂时先不使用散度的做法来计算这种接近程度，而是考虑使用一个分类器D来间接完成这种“散度”计算。可以直观地想象，假如高维空间中两个数据分布之间的距离较大，则可以很容易地找到一个超平面将这两类数据分隔开，而如果两个数据分布之间的间隙很小，几乎是同一个分布，则难以使用超平面将数据分开。鉴于此，目标转化为寻找参数$\theta$（也就是生成网络G）使得一个分类能力极强的分类器D无法分类生成数据和真实数据（此时要求D分类能力极强，因此在该场景下需要使用生成数据不断训练D使其分类能力提升，能分类生成数据和真实数据）。</p>
<p>根据以上分析，G和D都是一个函数，我们的目标是寻找一个生成函数G使得分类函数D无法分类生成数据和真实数据。<br>按照二分类的目标函数：</p>
<script type="math/tex; mode=display">
L(D) = P_{data}(x)log D(x) + P_G(x) log(1-D(x))</script><p>首先最大化$L(D)$，即寻找D使得分类效果最大，此处D是一个函数，随着真实数据$P_{data}$和生成数据$P_G$的不同取不同的最优分类解，因此，D与且仅与$P_{data}$和$P_G$有关。按照泛函求导，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-50-54.png" class=""><br>首先确定D，相当于不管生成函数G产生何种数据分布，都能直接按照公式$D(x) = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}$获得最优的分类器D，D已经和G直接挂钩，可以忽略了，只考虑G的取值问题（这里假设函数D能直接一步到位寻找到最优解，这是一个很强的假设，但实际操作的时候，这一步寻找最优的D需要靠G产生的数据和真实数据训练得到的，未必能逼近此时数学计算中理想的最优D），将D替换成G，此时问题转化如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-51-06.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-51-16.png" class=""></p>
<p>从实际操作来说，这一步需要使用生成函数G产生数据进行模型G的训练。前面说了，最优分类器D也需要使用生成数据和真实数据进行训练，从这里也可以看出，GAN的过程可以是生成函数G和分类函数D交替训练的过程。</p>
<p>从最终结果可以看到，最终GAN等价于计算某种散度（此处是JS散度），毕竟需要衡量两个分布之间的差距。前面按照直觉分析了计算分布差距可以转化为二分类问题，以上过程从数学的角度说明了，生成数据和真实数据的二分类在这个问题场景本质上是在逼近JS散度。</p>
<p>从一开始的分析中我们得到，生成模型需要寻找一个生成函数G使得$P_G$和$P_{data}$接近，即：</p>
<script type="math/tex; mode=display">
G = \mathop{\arg\min}_{G} \ \ div(P_G, P_{data})</script><p>后来又将$div(P_G, P_{data})$转化成使用生成函数D来衡量$P_G$$P_{data}$的可分程度，此处的D必须是当前最优分类器，即：</p>
<script type="math/tex; mode=display">
div(P_G, P_{data}) = \mathop{\arg \max}_{D} \ \ V(D, G) = \mathop{\arg \max}_{D} \ \ P_{data}(x)logD(x) + P_G(x)(1-log D(x))</script><p>因此，综合上面两个式子，我们其实是在求解：</p>
<script type="math/tex; mode=display">
G = \mathop{\arg \min}_{G} \ \ \mathop{\arg \max}_{D} \ \ V(G, D)</script><p>下面看一个简单的具体例子：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-52-17.png" class=""><br>假设目前的G函数只有三个，分别是G1，G2和G3。D函数有无穷多个，排列在x轴。则分别针对3个生成函数，寻找V(D,G)的最高分，然后选中这三个最高分中的最小值对应的生成函数G，即是G3作为求解结果。这里也可以看到一个明显的博弈过程，D要在3个G的场景下表现最好，即最优分类，而G的生成能力要最强，就要在3个D最优分类的基础上找最差分类，表明最优分类的D都能难区分G的生成结果。</p>
<p>算法的实做过程：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-52-29.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-52-40.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-52-56.png" class=""><br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-53-05.png" class=""></p>
<p>总结一下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-53-34.png" class=""></p>
<h3 id="3-Conditional-GAN"><a href="#3-Conditional-GAN" class="headerlink" title="3. Conditional GAN"></a>3. Conditional GAN</h3><p>原始的GAN虽然可以作为生成能力很强的生成模型使用，但由于输入是随机噪声，生成的结果可能不是我们预期要的效果，总体看并不可控。考虑在此基础上增加一个限制条件作为约束。例如不再随机产生图片，而是输入文字产生对应画面的图片。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-53-53.png" class=""><br>这个任务有点像图片字幕的对偶任务，一直最直接的思路是，使用文字和图片构成的数据集，端到端有监督地训练一个文字到图片的映射关系。但这么做是不够的，例如带有火车这个单词的样本很多，对应的标签中火车可能是不同的角度，因此最终得到的文字到图片的映射网络为了拟合总体的误差，会将火车输出为多种角度的平均角度，即一个模糊的火车。因此传统方法不适用。</p>
<p>可以在原始GAN的基础上改进，设计带（约束）条件的GAN，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-54-04.png" class=""><br>很明显地，生成函数不仅仅输入一个随机分布向量，也需要输入文字描述的语义向量，但是此时产生的图片作为假样本，真实图片作为真样本，最终会变成G只产生像真实图片的图片，不管图片和输入的描述文本是否匹配。所以，分类器D还需要知道生成函数G一开始接收的文字描述信息，因此输入应当包括文本和图片两项。真样本当然是文本加匹配的真图片，假样本是文本加假图片，此处假图片必须是不匹配的真图片和生成器生成的图片两者皆有。如果不加入不匹配的真图片，生成器G依然可以通过产生和文字描述无关的逼真图片，并且分类器D忽视文字输入而最终达到GD均衡，因此此处的假样本必须有文字加不匹配的真实图片。</p>
<p>下面是伪代码描述，总体过程相比原始的GAN多了文本输入和多一种假样本，其余不变。<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-54-16.png" class=""></p>
<p>至此，CGAN的网络架构和原始的GAN一致，区别仅在输入端和假样本。有一些文献设计了不同的架构，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-54-25.png" class=""><br>出发点应该是生成器G产生符合要求的样本可以分两步走，一个是产生的图首先必须逼真，然后再是和文字描述足够匹配。于是可以将生成器产生的图片分两个分支，一个约束逼真程度，一个约束匹配程度。其中，约束逼真程度的D可以和原始GAN一样直接分类图片就好，约束匹配程度的D，真样本是文字和匹配的真图，假样本是文字和生成图片，应该也还需要文字+不匹配的真图？总结来说这种架构的好处是利用分支任务监督生成真图，图片逼真的话loss开始减小，可能一定程度上缓解了原始的CGAN在生成文字+不匹配的真图时loss依然很高的问题。</p>
<p>例子：<br>如下图<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-54-34.png" class=""><br>指定建筑物的大致外观作为约束条件，要求将产生外观相近的带有细节的建筑物图片。<br>按照condition GAN的做法直接训练，但是产生的建筑物可能带有部分细节不太真实，例如最下方的第三个图中左上角有烟囱图形，考虑在condition GAN的原始loss上增加一个普通的图像均方差（当然，需要有平行数据），最终效果改善明显。另一个实做遇到的问题是，产生的图片尺寸较大，模型参数多，收敛时间长或者难以收敛，因为此时模型可能有办法产生细节上合理的图片但是整体构图有问题，此时仍然得不到好的反馈，因此引入分层的想法，即使用小的判别器判断局部细节是否足够真实，大的判别器判断整体构图是否和谐，如下：<br><img src="/2020/02/06/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/2020-02-06-22-54-44.png" class=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/" itemprop="url">各样各式的GAN——infoGAN, VAE-GAN, BiGAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T22:34:11+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是李宏毅老师的公开课的学习记录，全部图片来自李老师的课件。文字按照个人理解做了组织和阐释。主要介绍了几种GAN——infoGAN, VAE-GAN, BiGAN。</p>
<h3 id="infoGAN"><a href="#infoGAN" class="headerlink" title="infoGAN"></a>infoGAN</h3><p>普通的GAN中，生成函数G的输入是一个随机向量，输出是一个图片（例如手写数字）。实践中发现，这个随机向量的维度没有实际物理含义：线性地改变向量的某个维度的数值，然后输入到生成函数G，获得的图片并没有呈现出某个物理层面的渐变效果，例如笔画的粗细，数字的倾斜程度等等。如下：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-37-41.png" class=""><br>这个问题的一个可能的解释是：从人自身的角度出发，我们认可笔画的粗细，数字的倾斜程度，颜色等作为特征，渐进地改变某个维度的数值时，就像图中右下角的坐标一样，会经历颜色属性的改变，但对于生成函数来说，它认为的特征可能是另一种人类不熟悉的属性，当渐进地改变这种属性时，转换为图片之后人看到的特征就像是多类特征的杂合，甚至没有规律。<br>如果能解决这个问题，将使得生成模型的效果较为可控，infoGAN就是为了解决这个问题被提出的。如下：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-37-50.png" class=""><br>在普通GAN的基础上做出的改动是：输入的向量拼接多一项c，同时增加一个分类器，从生成的图片中将c重新解析出来。为了避免生成函数G直接把c的信息明显地放在生成图片中，因此必须保证生成图片是正常的，故不能拆掉分类器D。</p>
<p>infoGAN这种做法的出发点是，如果在输入端额外增加了某种信息c，而这种信息没有被分类器D发现，同时解析器C还能从图片中解析出信息c（假如解析器C的脑补能力并没有很强），那么这种信息只能来自于图片的内容，因此这种做法会强迫信息c去影响图像的内容中，通过内容将信息传递给解析器C，这种影响可能就是通过影响一些属性（笔画粗细等）达成的，因此此时的向量c有可能具备渐进地控制生成图像内容的能力。以上要求解析网络C的能力不能太强，因为如果解析网络C的能力很强，可能C和G会合作找到一种接头暗号，将信息c简单地放在图像的某个小区域，同时骗过分类器D的底层参数，使其发现不了（可能信息c藏得很散乱或者是很微弱，D发现不了但是C的能力很强可以解析出来）。因此需要想办法限制一下C网络的能力，结合这个场景，一个较好的方法是让解析网络C和分类器D尽可能多地共享参数，另外地，C和D都是接收生成图像，因此它们的底层网络本来也应该是一样的。</p>
<p>效果：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-38-09.png" class=""><br>从第一张图的结果看，信息c的第一维控制数字；从第三张图看，第二维度控制倾斜角度；从第三张图看，第三维度控制笔画的粗细。</p>
<h3 id="VAE-GAN"><a href="#VAE-GAN" class="headerlink" title="VAE-GAN"></a>VAE-GAN</h3><p>顾名思义，将VAE和GAN结合起来。这种做法的出发点是原始VAE应用在图像中是使用像素均方差作为loss函数，这种做法生成的图像往往很模糊，因此考虑引入GAN做改进。此时，VAE的decoder作为生成函数G，后接一个分类器D，总体结构如下：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-38-24.png" class=""><br>需要注意的是这里不仅仅把autoencoder重构出来的图像x作为假样本，还需要使用随机的向量输入decoder解码获得的图像作为假样本。真样本就只有真实图片这一种。<br>算法流程如下，第五个小点中$\hat{x^i} = En(z^i)$改为$\hat{x^i} = De(z^i)$，其余没有什么需要补充的。<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-38-33.png" class=""></p>
<h3 id="BiGAN（Bidirectional-GAN）"><a href="#BiGAN（Bidirectional-GAN）" class="headerlink" title="BiGAN（Bidirectional GAN）"></a>BiGAN（Bidirectional GAN）</h3><p>BiGAN是另一种改进思路。分开学一个encoder和decoder，和autoencoder中的encoder和decoder的作用相同，encoder吃一个图片，获得一个编码向量z，decoder吃一个编码向量z（随机产生），获得一个图片。整个过程看起来和autoencoder很相似，唯一的不同是encoder输出的z并不会直接输入到decoder中，所以在这一点上encoder和decoder并没有什么联系，而是通过另一个分类器D来建立联系。如下图：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-38-43.png" class=""><br>分类器D在这里需要分开的对象是encoder的输入输出构成的整体和decoder的输入输出构成的整体。训练的最终目的当然就是让分类器D分不开，这时候encoder的输入输出和decoder的输出输入可以认为是接近的，这时再把encoder和decoder拼接起来作后续的使用。</p>
<p>按照李宏毅老师的理解，这种做法和直接做autoencoder在达到最优解的情况下是一样的，但其实无法达到最优解，因此两种做法最终的结果有不同。如下：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-38-56.png" class=""><br>我想其实可以从GAN对图像清晰程度的提升这一点来分析，普通autoencoder对图像的重构使用像素均方差，将decoder单独拿出来使用会出现生成图像模糊的问题，BiGAN中分类器最终的目标是分不清真实encoder和decoder的输入和输出，可能可以推导到分不清真实图像和生成图像，这会提升decoder的生成图像质量。<br>实践中BiGAN的效果的部分示意图如下：<br><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-39-40.png" class=""><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-39-07.png" class=""><img src="/2020/02/06/%E5%90%84%E6%A0%B7%E5%90%84%E5%BC%8F%E7%9A%84GAN/2020-02-06-22-39-58.png" class=""><br>和普通autoencoder得到模糊的图像不同，BiGAN得到的图像纹理比较清晰，但是整体看起来不太像真实图片==。另外，BiGAN的结果有一个特点是，例如输入一个鸟的图片，重构结果也是类似鸟的图片但角度各方面都不会和原图一致。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" itemprop="url">遣词造句的规律，人类语言的奥秘——语言模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T22:10:32+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%B6%E4%BB%96/" itemprop="url" rel="index">
                    <span itemprop="name">其他</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>普遍意义上的自然语言处理中的语言是指人类之间沟通交流的工具，具体形式上可以有文本和语音等多种表现形式。语言携带信息的方式随着人类群体的的不断进化和发展而逐渐变得丰富，同时也变得极为复杂，以至于到了近现代社会人类自身都很难概括出理解文本语义信息需要经历的逻辑变换过程。加上现代计算机体系和人脑运作体系的差别较大，人类难以将自身对语言的理解过程直接转化为计算机程式，因此演化出了自然语言处理这个计算机科学的子领域。自然语言处理的主要目的是通过统计学习的方法使计算机能接收和处理文本，从中提取出文本表达的信息。其中，语言模型是自然语言处理的基础。本文将介绍语言模型的相关理论和一些重要工作。</p>
<h3 id="语言模型定义与求解"><a href="#语言模型定义与求解" class="headerlink" title="语言模型定义与求解"></a>语言模型定义与求解</h3><p>一个句子S通常被看作由一系列词语元素$w_i$构成的有序序列，即$S=w_1w_2w_3…w_N$。语言模型认为某种语言体系下的所有句子构成一个概率分布，每个句子对应的概率$p(S)$表示该句子的出现概率。<br>语言模型的基本任务就是估计每个句子的出现概率，如公式\eqref{eq:lm_ps}所示：</p>
<script type="math/tex; mode=display">
P(S) = P(w_1w_2w_3...w_N)</script><p>利用条件概率，可以将公式转换如下：</p>
<script type="math/tex; mode=display">
P(w_1w_2w_3...w_N) = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)…P(w_N|w_1w_2w_3…w_{N-1})</script><p>因此，根据该公式，要估计一个句子$S$的出现概率，只需要预先在一个大型语料库中，分别统计出条件概率$P(w_i|w_1w_2w_3…w_{i-1})$，再将所有结果累乘即可。该过程利用了伯努利大数定理（当频次<strong>足够高</strong>时，频率趋近于概率），如下面公式所示：</p>
<script type="math/tex; mode=display">
P(w_i|w_1w_2w_3…w_{i-1})\approx \frac{\text{Count}(w_1w_2w_3…w_{i-1}w_i)}{\text{Count}(w_1w_2w_3…w_{i-1})}</script><p>在实际操作中，当$i$较大时，句子$S$的前$i$个元素构成的前缀子序列$S_i=w_1w_2w_3…w_{i-1}w_i$往往非常稀疏，导致大数定理失效，因此很难直接使用公式\eqref{eq:lm_pwi}估计条件概率$P(w_i|w_1w_2w_3…w_{i-1})$。Jelinek<sup><a href="#fn_Jelinek1980interpolated" id="reffn_Jelinek1980interpolated">Jelinek1980interpolated</a></sup>提出一种近似算法。基于Markov假设，即子序列$w_1w_2w_3…w_{i-1}w_i$中，$w_i$元素只依赖于前$k$个元素，故公式\eqref{eq:lm_pwi}可以改写为：</p>
<script type="math/tex; mode=display">
P(w_i|w_1w_2w_3…w_{i-1}) = P(w_i|w_{i-k}…w_{i-2}w_{i-1})\approx \frac{\text{Count}(w_{i-k}…w_{i-2}w_{i-1}w_i)}{\text{Count}(w_{i-k}…w_{i-2}w_{i-1})}</script><p>一般情况下，$k$取值定为2。利用上面的公式进行语料库统计不仅可以解决子序列前缀子序列$S_i$稀疏的问题，较短序列的匹配也使得统计过程的计算量大大降低。</p>
<h3 id="神经语言模型"><a href="#神经语言模型" class="headerlink" title="神经语言模型"></a>神经语言模型</h3><p>传统的统计语言模型使用上面最后一个公式所示的方法进行求解。该方法在大多数任务场景下表现良好，但是当给定的句子中某些序列片段未在语料库出现或者出现次数较少时，针对该部分的条件概率估计将会出现较大偏差。过去的研究工作设计了多种平滑方法<sup><a href="#fn_katz1987estimation" id="reffn_katz1987estimation">katz1987estimation</a></sup>来针对统计频数较少的序列片段进行平滑修正，但通过人工规则定义的平滑算法有较大的适用局限。同时，由于统计语言模型基于Markov假设，这导致模型只关注目标元素之前的若干元素，无法和更多的元素建立长时依赖关系。为了解决上述问题，Mikolov等人<sup><a href="#fn_mikolov2010recurrent" id="reffn_mikolov2010recurrent">mikolov2010recurrent</a></sup>提出了基于循环神经网络的语言模型RNNLM。其模型结构示意图如下图所示：<br><img src="/2020/02/06/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/2020-02-06-22-28-02.png" class=""><br>RNNLM将<strong>循环神经网络作为一个在设定误差之内能拟合任意连续函数<sup><a href="#fn_nn_as_func_appro" id="reffn_nn_as_func_appro">nn_as_func_appro</a></sup>的工具</strong>，用来端到端地学习一个<strong>从前缀子序列$S_{i-1}=w_1w_2w_3…w_{i-1}$到条件概率$P(w_i|w_1w_2w_3…w_{i-1})$的映射关系</strong>。下面以LSTM为例阐述神经语言模型的数学描述，如下两个公式所示：</p>
<script type="math/tex; mode=display">
h_t = \text{LSTM}(h_{t-1}, x_t)</script><script type="math/tex; mode=display">
p_t = \text{softmax}(W^{\text{LM}} h_t)</script><p>公式中的$\text{LSTM}$函数表示对于给定的序列元素$x_t$和$h^x_{t-1}$，使用LSTM的迭代公式计算出$h^x_{t}$。特别地，对于0时刻的$h^x_{0}$一般直接初始化为一个固定向量，神经语言模型中采用零向量。公式$p_t$将时刻$t$的LSTM网络输出投影到下一个元素的预测概率空间上，按照极大似然思想构建损失函数训练该模型，如下公式所示：</p>
<script type="math/tex; mode=display">
loss_{LM} = - \frac{1}{|D|}\sum _{S \in D} \sum_{t=1}^N V_{w_{t+1}} \text{log } p_t</script><p>其中$V_{w_{t+1}}$表示第$t+1$个词典单词对应的one-hot向量。句子序列$S=w_1w_2w_3…w_N$是训练集$D$的一个样本实例。总的来看，该公式是使用交叉熵函数计算句子序列$S$的拟合误差，然后按照极大似然估计的思想将训练集的所有样本误差求和，作为整个训练集的拟合误差。最后按照梯度下降法对上述式子进行优化，得到最终的神经网络语言模型。</p>
<h3 id="语言模型和句子可读性"><a href="#语言模型和句子可读性" class="headerlink" title="语言模型和句子可读性"></a>语言模型和句子可读性</h3><p>一个句子可以看做是由词或字等基本元素构成的有序序列。句子的可读性从直观上来说是其元素排列符合语言语法的程度，本质上说是句子的出现概率。因此可以考虑使用语言模型来对一个句子的可读性进行打分。</p>
<p>神经语言模型如上图所示，输入的句子序列是“天气好。”，加上起始符号$\text{<sos>}$一起作为输入序列，每个元素依次输入，然后依次预测下一个元素。一般来说，一个训练好的语言模型能正确预测下一个元素。如上图所示，当输入元素为“天”时，对应的输出元素应当是“气”，此时输出概率分布中，最大概率的输出元素是红色部分对应的词典元素“气”。该句子的可读性即是将所有红色部分的概率累乘起来，表示这句话的通顺程度。</p>
<h3 id="tips"><a href="#tips" class="headerlink" title="tips"></a>tips</h3><ol>
<li>神经语言模型中使用的RNN单元必须是单向的，因为训练过程是使用前文序列预测下一个单词的过程。假如使用双向RNN的话模型会利用到前文序列之后的信息，自然可以作弊得到前文序列的下一个单词。</li>
<li>语言模型的评估指标常用困惑度，按照定义，这等价于句子可读性计算中提到的概率乘积的倒数。在实际操作中，概率累乘的过程可能会出现浮点数过小溢出，为了安全起见可以转化成对概率取对数再相加，最后再对加和的结果取指数。</li>
</ol>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><blockquote id="fn_mikolov2010recurrent">
<sup>mikolov2010recurrent</sup>.  Mikolov T, Karafiát M, Burget L, et al. Recurrent neural network based language model. Eleventh annual conference of the international speech communication association, 2010.<a href="#reffn_mikolov2010recurrent" title="Jump back to footnote [mikolov2010recurrent] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_katz1987estimation">
<sup>katz1987estimation</sup>.  Katz S. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing, 1987, 35(3):400–401.<a href="#reffn_katz1987estimation" title="Jump back to footnote [katz1987estimation] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_nn_as_func_appro">
<sup>nn_as_func_appro</sup>.  Hornik K. Approximation capabilities of multilayer feedforward networks. Neural networks, 1991, 4(2):251–257.<a href="#reffn_nn_as_func_appro" title="Jump back to footnote [nn_as_func_appro] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_hochreiter1997long">
<sup>hochreiter1997long</sup>. Hochreiter S, Schmidhuber J. Long short-term memory. Neural computation, 1997, 9(8):1735–1780.<a href="#reffn_hochreiter1997long" title="Jump back to footnote [hochreiter1997long] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_cho2014learning">
<sup>cho2014learning</sup>.  Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.<a href="#reffn_cho2014learning" title="Jump back to footnote [cho2014learning] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_schuster1997bidirectional">
<sup>schuster1997bidirectional</sup>.  Schuster M, Paliwal K K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 1997, 45(11):2673–2681.<a href="#reffn_schuster1997bidirectional" title="Jump back to footnote [schuster1997bidirectional] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_graves2005framewise">
<sup>graves2005framewise</sup>.  Graves A, Schmidhuber J. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 2005, 18(5-6):602–610.<a href="#reffn_graves2005framewise" title="Jump back to footnote [graves2005framewise] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_Jelinek1980interpolated">
<sup>Jelinek1980interpolated</sup>.  Jelinek F, Mercer R L. Interpolated estimation of markov source parameters from sparse data. Pattern Recognition in Practice, 1980.<a href="#reffn_Jelinek1980interpolated" title="Jump back to footnote [Jelinek1980interpolated] in the text."> &#8617;</a>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/" itemprop="url">ICML2018演讲记录-深度学习基础理论</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T21:52:25+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%90%86%E8%AE%BA/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习相关理论</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是记录一下 <strong>ICML2018</strong> 上的演讲，讲解的东西比这个稍多，按照分类目录，将深度学习的基本理论相关的部分放在本文。</p>
<h3 id="尝试理解网络"><a href="#尝试理解网络" class="headerlink" title="尝试理解网络"></a>尝试理解网络</h3><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-57-20.png" class="">
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-57-30.png" class="">
<p>从优化的角度来看，深度网络作为一个映射工具，将输入映射到输出，在输出端，使用一些衡量差距的算式，计算出输出和期望输出的差距，然后用优化方法来矫正输出，使得输出和期望的输出接近。<br>不过，这种优化和传统的凸优化是不同的，基本上，我们面对的问题常常是很难保证是凸的，常规的优化器容易导致搜索到局部最优点。（可能经过设计的优化器有概率会跳过局部最优，前往更优点？）<br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-57-40.png" class=""><br>讨论的点如上图：</p>
<ol>
<li>面对一个非凸问题，如何使用梯度下降法或者其他变形方法寻找更优点？</li>
<li>深度网络的参数量常常是很大的，而传统的一些统计的方法告诉我们，参数量远大于样本数量时，基本会带来严重的过拟合问题，但是，现在我们看到，网络的方法泛化能力很强。</li>
<li>深度网络中，深度到底起什么作用？</li>
<li>探讨一些关于无监督的方法和GAN</li>
<li>是否有更简单的方法取代部分网络的模块，这种简单是指我们知道他的原理，可以方便的根据实际使用效果作分析和改进。</li>
</ol>
<h4 id="1-optimization-problem"><a href="#1-optimization-problem" class="headerlink" title="1. optimization problem"></a>1. optimization problem</h4><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-57-57.png" class="">
<p>深度网络中的优化问题多数属于非凸。<br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-58-07.png" class=""><br>实际上，很难设计优化器去跳出局部最优，朝着全局最优去进化。<br>我们想另一个问题，放弃一般性问题，即放弃随机初始化，是否能找到一些起始点，能证明从这些起始点开始迭代就能达到全局最优。<br><strong>高维度的诅咒</strong><br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-58-19.png" class=""><br>关于高维，有一个比较违背直觉的结论：<br>针对一个$d$维度的空间，在其中一定能找到数量级在$e^d$的若干个具体的向量，这些向量中，任意两个之间的夹角大于60度。<br>由此推出另一个更加普适的结论是：<br>针对一个$d$维度的空间，在其中一定能找到数量级在$e^{d/\epsilon }$的若干个具体的向量，这些向量中，任意两个之间的夹角大于$\epsilon$度。<br>这除了说明高维空间非常大之外，其实更重要的，假如我们按照一个精度$\epsilon$将空间离散为若干份，其中一个是最优方案，即参数空间中使得loss最小的那个最优解，我们寻找到这个最优解的过程是很艰难的，因为离散之后的向量数量级高达$e^{d/\epsilon }$。<br>如果我们能具体地知道参数空间和loss之间的映射关系构成的超平面，则可能不需要在一个高达$e^{d/\epsilon }$空间中穷尽地搜索，但实际上这个超平面是真的不知道。<br>另外补充一点：关于深度网络的黑盒。如果单纯从输入到输出，输出端用一个优化方法这一点来看，深度网络倒是完全不像黑盒。我们平时说的深度网络是一个黑盒，说的是这个超平面很难知道，即从输入到输出很难把握他的数学性质，也就无法求解参数，最后选择了忽视这些性质，直接做任务驱动的端到端求解。</p>
<p>至此，我们承认，现在没有什么好方法能在一个未知的超平面中寻找全局最优解。</p>
<p>下面简单分析一下在一个未知的超平面中做梯度下降法的问题。<br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-58-45.png" class=""><br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-58-54.png" class=""><br>我们经常使用的梯度下降法是计算出loss函数$f(\theta) $的梯度$\theta _t$，然后执行$\theta _{k} = \theta _{k-1} - \eta \Delta f(\theta)$更新参数值。这样做的道理是根据梯度的反方向可以知道函数下降的方向，但是不确定需要走多大的步伐，因为在极小的一个局部，梯度反方向可以确定函数的下降方向，但是步子迈得太大，迈出了该区域则可能函数反而是上升的。可以证明，应该迈多大的步伐取决于海塞矩阵的边界。直观理解就是，二阶导数决定了一阶导数的变化程度，而在极小的区域内，一阶导数可以近似函数的曲线幅度，如果二阶导数较大，将导致本次的一阶导数较大，一个较小的学习率$\eta$就可能导致函数横坐标超出有效的小区域。所以学习率应该是和二阶导数呈负相关的关系。</p>
<p>以上是基础做法，可能带来的问题其实也很多，比如：</p>
<ol>
<li><p>鞍点问题：</p>
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-59-05.png" class="">
<p>鞍点的示意图如上图所示。以上做法实际上在求取梯度为0的点，鞍点同样是梯度为0的点，但是可以看到，鞍点并不是一个局部最小值。<br>paper《add noise to gradient》提出了一种能对付鞍点问题的方案。扰动的梯度下降法(perturbed gradient descend)<br>实际上，我们不需要在训练的时候引入噪声的，因为基于batch的随机梯度下降法本身就会带来不确定噪声，这些噪声作为扰动，已经可以帮助优化器脱离鞍点。</p>
</li>
<li><p>速度问题<br>我们知道，一阶优化的收敛速度慢于二阶优化器。二阶优化在传统的一些图优化问题其实是用的比较多的了，那么对于深度学习是否也可以直接设计高阶优化器呢？</p>
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-59-19.png" class="">
<p>二阶优化方法也称为牛顿法(Newton Method)。<br>实际上，原始的牛顿法是来自于方程求根问题。<br>考虑方程求根问题：即要求$f(x)=0$的解。<br>数值球迭代求解中，需要先做近似，考虑在点$x_0$附近做一阶的泰勒展开：</p>
<script type="math/tex; mode=display">
f(x)\approx f(x_0)+f'(x_0)(x-x_0)=0</script><p>故$x = x_0 - \frac{f(x_0)}{f’(x_0)}$，由此可以得到迭代式：$x_k=x_{k-1} - \frac{f(x_{k-1})}{f’(x_{k-1})}$，迭代式需要给定一个初始值$x_0$，初始值不需要严格限定范围。<br>可以看到，从理论数学的角度看，牛顿法需要保证取到的点都有导数值并且导数值不为0，这需要知道函数f的具体导数，才能完成求解。而实际操作中，可能并不知道函数f的导数形式，无法代入具体的$x_{k-1}$去计算导数值，这种情况下常常会使用一个近似处理，即使用前两次计算的点来近似计算导数，故迭代式变为：$x_k=x_{k-1} - \frac{x_{k-1}-x_{k-2}}{f(x_{k-1})-f(x_{k-2})} f(x_{k-1})$，这被形象地称为’割线法’。<br>我们知道，优化loss函数$g(x)$的时候，是在寻找一个参数值使得loss处于极小值点，即$g’(x)=0$，考虑使用牛顿法求解的话，此处的$g’(x)=f(x)$，故迭代式为：</p>
<script type="math/tex; mode=display">
\begin{equation}
x_k=x_{k-1} - \frac{g'(x_{k-1})}{g''(x_{k-1})}
\label{1}
\end{equation}</script><p>考虑到x是高维向量的情况，加上一个学习率$eta$，则式子(1)变为：</p>
<script type="math/tex; mode=display">
x_{k} = x_{k-1} - \eta {\Delta ^2 f(x)}^{-1} \Delta f(x)</script><p>这就是二阶方法做优化的迭代式。<br>这里面涉及到的计算，一个是梯度，这个我们在一阶优化中也需要算，另一个是二阶导数的导数，在这里体现为海塞矩阵的逆阵。94年，Paul等人证明了海塞矩阵可以在线性时间内求解，但是这里需要的是海塞矩阵的逆阵，当维度较高时这是一个非常棘手的问题。后来，又陆续有工作证明了，可以使用一些近似的方法来高效地计算海塞矩阵的逆阵（思路应该类似于割线法？）。这让二阶优化看到了新希望。<br>但是非常遗憾，人们在尝试了二阶优化器之后，发现训练出来的深度网络的效果要差于普通的一阶梯度下降方法训练出来的结果。<br>下面分别附一个随机梯度下降法和一个牛顿法的示意图，全部源自网络。</p>
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-21-59-41.png" class="">
<p>(SGD示意图)</p>
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-00-02.png" class="">
<p>（牛顿法示意图）</p>
</li>
<li><p>尝试打开黑盒</p>
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-01-21.png" class="">
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-01-39.png" class="">
<p>18年有paper《Spurious Local Minima are Common in Two-Layer ReLU Neural Networks》证明了，针对比较简单的单隐层全连接网络，如果使用ReLU激活函数的话，那么当各层的维度$k$处于$6 \lt k \lt 20$时，难以保证参数平面是凸的，但是当k超过这个范围时，特别是当k较大时，是能这证明该参数平面的局部最优等价于全局最优的。从这个角度看，其实我们已经能打开普通的三层神经网络这个黑盒了。<br>但是，对于更多层的深度网络，目前没有新的研究进展。当前的研究最多的是停留在假设没有激活函数的情况，但是这样其实研究意义不大。因为不带有激活函数的多层神经网络其实也就等价于单层。</p>
</li>
</ol>
<h4 id="3-overparameterization-and-generalization-theory"><a href="#3-overparameterization-and-generalization-theory" class="headerlink" title="3. overparameterization and generalization theory"></a>3. overparameterization and generalization theory</h4><ol>
<li>加多参数量降低优化难度<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-01-54.png" class="">
有一个有趣的现象：如上图，假如随机初始化一个深度网络，然后随机输入一些向量，产生一些输出，当做labels，然后将这些输入和对应的输出固定为训练pair。此时使用另一个一模一样结构的网络，只是参数随机初始化，这时会发现，要拟合这些pair常常是很困难的。但是如果增加多一点参数，将使得拟合顺利得多。<br>另一个现象如下图：<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-02-05.png" class="">
我们想象中，随着训练次数的增加，训练集的loss会一直下降，但是测试集或者验证集的loss将应为随之而来的过拟合的发生，会先下降后上升。但实际中，对于深度网络，过拟合似乎不是那么明显。<br>关于这一点，其实传统的线性模型也有。<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-02-15.png" class="">
可以证明，线性模型的‘容量’正比于模型参数数量d的对数，反比于模型设定的距离y的平方。<br>衡量模型容量的方法：<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-02-31.png" class="">
<img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-02-39.png" class="">
</li>
</ol>
<h4 id="4-role-of-depth"><a href="#4-role-of-depth" class="headerlink" title="4. role of depth"></a>4. role of depth</h4><p>和前面一篇博文《深度学习中网络拟合能力和网络深度的理论分析》雷同，就不浪费篇幅了。。</p>
<h4 id="5-generative-model-and-GAN"><a href="#5-generative-model-and-GAN" class="headerlink" title="5. generative model and GAN"></a>5. generative model and GAN</h4><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-02-54.png" class="">
<p>一开始人们考虑生成模型的时候，是直接重构图片，这样导致了生成的图片非常模糊，而且图片与图片之间的插值不是正常的图片，产生了严重的重影。以人脸生成为例，中间插值的图片可能是多张人脸的叠加，模糊而且看起来很诡异，并不是真正的产生了人脸。<br><img src="/2020/02/06/icml2018%E6%BC%94%E8%AE%B2%E8%AE%B0%E5%BD%95/2020-02-06-22-03-03.png" class=""><br>于是一个新的思路是GAN。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/" itemprop="url">极大似然估计和最大后验概率估计的深入理解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T21:33:58+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>进行贝叶斯估计的时候，我们利用贝叶斯公式将$P(class|x)$的问题转化成了$P(x|class)P(class)$的问题。其中类条件概率$ P(x|class)$的估计是重点。<br>$P(x|class)$由参数$\theta$确定，则类条件转化成求取$P(x|\theta)$</p>
<p>关于$P(x|\theta)$的求解，从上世纪二三十年代开始就有争论。主要流派为频率主义学派和贝叶斯学派。<br>频率主义学派认为参数估计中，参数未知，但是一个固定的值，通过优化似然函数可以求解。<br>贝叶斯学派认为参数未知，而且是一个符合某种分布的变量，可以假定参数服从一个先验分布，然后通过观测训练集，获得参数的后验分布。<br>这两种分别对应下文的两种方法。</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>平时我们经常用到该方法。训练模型的时候假定模型参数，然后计算出每个样本的概率，依照最大似然的原则，优化训练集样本乘积为最大，即$P(X|\theta)$为最大，由此求取参数值。</p>
<h3 id="最大后验估计"><a href="#最大后验估计" class="headerlink" title="最大后验估计"></a>最大后验估计</h3><p>最大后验概率则认为参数本身存在一个先验分布，故优化目标不仅仅是似然函数，因为参数先验分布是前提，故优化目标是参数先验分布乘上似然函数，其实非常好理解。<br>这部分也就是贝叶斯公式右边的分子部分，故称为后验概率。即优化$P(\theta|X)$为最大。<br>贝叶斯学派主张优化后验概率至最大，由此求得参数θ。</p>
<p><strong>举个例子：假设现在抛了5次硬币，有4次朝上，1次朝下，则第6次硬币朝上的概率？</strong></p>
<p><strong>极大似然估计：</strong><br>按照理解，直觉想即是，4/5<br>追究具体过程的话可以是：<br>假定朝上概率是$\theta$，则极大似然概率$P=\theta^4(1-\theta)$，寻找$\theta$使得P最大，简单可知$\theta=0.8$时P最大。</p>
<p><strong>最大后验估计：</strong></p>
<ol>
<li>按照先验，人主观认为硬币是均匀的，则朝上概率最可能是0.5。此处如果我们选定先验的函数形式是高斯函数，则0.5应该定为均值，符合$\theta$最可能是0.5的先验。<br>则：<br>$P(\theta)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(\theta-0.5)^2}{2\sigma^2}}$<br>暂时取定$\sigma=0.1$<br>优化$P(X|\theta)P(\theta)$为最大，则是等价于优化$C_5^4\theta^4(1-\theta)P(\theta)$，借用matlab画出该曲线，可以看出$\theta$取值大约是0.7<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-12.png" class="">
此时对比极大似然估计可以看出，先验本来是0.5，但是发生的5次抛硬币事件使得先验得到修正，朝着0.8的方向修正，变成了0.7，假如试验的次数继续增多，例如增加到500次，其中400次朝上，则如下图：<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-26.png" class="">
可以看出，多次的事实最终使得先验被彻底修正，$\theta$取值无限逼近0.8。<br>此时如果我们再想深入一步：假如抛了很多次硬币，发现正反的次数相等，这个高斯分布的假设能否求出$\theta=0.5$呢，即$C_{2M}^M\theta^M(1-\theta)^MP(\theta)$不管M取值多少，最大值都是在$\theta=0.5$取得吗？明显很难有这么巧的事情，例如M=5时，图像如下，最大值不是在$\theta=0.5$处取得，而且大体的规律是M越小最大值偏离0.5越多。<img src="/2020/02/06/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/2020-02-06-21-43-41.png" class="">
这说明用高斯函数的形式来拟合参数$\theta$的分布，这种做法本身就不自洽。</li>
<li>我们这里暂时先开一个上帝视角，直接引入一个二项分布。二项分布问题现在普遍采用的参数先验是Beta分布，该分布不存在上面说的自相矛盾的问题。<br>$ Beta(\alpha, \beta)=\frac{p^{\alpha-1}(1-p)^{\beta-1}}{B(\alpha, \beta)}$中$\alpha,\beta$只要等比例增加，则Beta曲线的最高点横坐标依然不变。<br>按照前人的总结，Beta分布式二项分布的共轭先验，该分布可以使得先验概率和和后验概率拥有相同的分布形式，即称为共轭。(百度百科对‘共轭’这个词条的解释：两头牛背上的架子称为轭，轭使两头牛同步行走。共轭即为按一定的规律相配的一对。 <strong>通俗点说就是孪生。</strong> )<br>那现在就有两个问题：<br>a.使用Beta分布的形式做参数$\theta$的先验是否就客观。<br>b.是否还存在其他形式的共轭先验分布。<br>关于这两个问题，按照自己的理解，尝试解答一下：<br>假如我们事先并不知道参数的分布，暂定为均匀分布，则后验概率：<br>$P(\theta|x)=P(x|\theta)=\frac{C_n^x\theta^x(1-\theta)^{n-x}}{B(x)}$<br>其中分母是为了归一化，保证概率密度函数的积分为1.<br>由此可以看出，后验概率的形式是Beta分布的形式。同时，考虑到此处的后验概率会成为下一次计算的先验概率(例如，下个回合继续抛10次硬币，是6次朝上4次朝下，则分两个回合计算和两个回合一同计算的结果应该是一样的。)，由此推导出先验分布应该和后验分布是同一个分布。因此，先验分布必须是Beta分布。<br>理清楚了这些，回到刚刚的例子，现在使用Beta分布来完成最大后验概率计算。<br>首先确定先验是$Beta(\alpha, \beta)$，发生了4次朝上，1次朝下的实验之后，后验变成了$Beta(\alpha +4, \beta +1)$，<br>一般而言，我们预想中的硬币是均匀的，即$\alpha = \beta$，故先验中，硬币朝上的概率是$P=\frac{\alpha}{\alpha + \beta}=\frac{1}{2}$，后验中，由于发生了5次实验，增加了信息，硬币朝上概率得到修正，修正为$P’=f(\alpha)=\frac{\alpha +4}{\alpha + \beta +5}=\frac{\alpha +4}{2\alpha +5}$，可以看到，明显的：<br>$f(\alpha) , \alpha ∈ Z$ 是单调递减的，同时$0.5 &lt;  f(\alpha) &lt;= 0.8$，左右端点分别在$\alpha$取值为$\infty$和0时取得。由此可见，最终的结果还取决于$\alpha$的大小，实际上，按照上文所言，先验是上一步计算的后验，所以，这里的所谓Beta先验，其实也可以视为是从0先验，然后进行了$\alpha + \beta$次实验，得到了$\alpha$次朝上和$\beta$次朝下的实验结果，然后获取到的后验知识。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>理解极大似然和最大后验的区别和联系，需要理解好先验和共轭先验。最后继续用上面的例子做一个总结：那硬币的例子来说，我们出生的时候，对硬币一无所知，即$Beta(0,0)$，随后一系列的生活经验，告诉我们$Beta(\alpha, \beta)$，这里的$\alpha$和$\beta$按照生活经验是基本相等。同时，$f(\alpha)$是一个递减函数也很好理解，假如我们从小看到的抛硬币实验数量太多而且都是接近0.5的结果的话，本次实验区区5次的结果更容易被认为是巧合，难以撼动之前的经验，而如果之前见过的抛硬币实验极少，对”硬币”这种物品没什么概念的话，则本次的5次实验很容易让人接受”硬币”其实就是一种朝上概率接近0.8的物品。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/" itemprop="url">高维场景下的维度灾难与高维统计方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T21:09:41+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是关于高维情况下的一些场景特征。主要是从两个论文入手介绍高维空间的一些特点。本质上，神经网络的参数空间极为庞大，也拥有高维空间的一些特点和问题。本文第一篇论文阅读其实不算高维问题，就算一个引入，着重介绍第二篇论文。</p>
<h5 id="1-High-dimensional-Feature-and-Its-Efficient-compression-for-face-verification"><a href="#1-High-dimensional-Feature-and-Its-Efficient-compression-for-face-verification" class="headerlink" title="1. High-dimensional Feature and Its Efficient compression for face verification"></a>1. High-dimensional Feature and Its Efficient compression for face verification</h5><p>CVPR2013，微软孙剑组的人脸识别论文，主要提出了一种获取高维的特征描述子的方法，并对描述子进行降维，便于计算特征点的相似度。其实这不太像严格意义的高维统计。<br>算法：<br>文章从人脸识别的基本步骤入手：提取特征描述子和根据特征计算人脸相似度。从提取描述子入手，按照经验，越多的特征点可以使效果提高，使用现有的成果，dense facial lankmark with alignment，外加多尺度的金字塔变换得到的描述子，一起拼接起来，得到一个高维的描述子(100k dimension)，接下来需要进行降维和并计算向量相似度。<br>降维分为两个步骤：</p>
<ol>
<li>先对描述子$P$进行PCA，去除部分噪声干扰，得到$P$；</li>
<li>对$Y$进行进一步降维，因为需要存储一个很大的降维矩阵，为了test阶段的存储和计算方便，人工约束这个转换矩阵稀疏。因此，得到优化目标：<script type="math/tex; mode=display">
min_B |Y-B^TX|_2^2+\lambda |B|</script>可以看到，优化目标分为两个，一个是重构误差，一个是降维矩阵B的稀疏约束，可以视为一个正则化。<br>上式要求降维矩阵的稀疏性，这个约束可能是过强的。另外，实际上，由于常用的欧拉距离，余弦距离等距离度量方式都有旋转不变性，故可以考虑在$Y$上加多一些自由度，即加上一个旋转矩阵，增多一些可能性，对冲矩阵$B$带来的约束。上式变为：<script type="math/tex; mode=display">
min_B |R^TY-B^TX|_2^2+\lambda |B|</script>上式相比于1式，增加多了一个旋转自由度，在原paper中自称为Rotated Sparse Regression。</li>
</ol>
<p>求解：<br>求解这个目标函数。使用梯度下降的普遍解法进行优化可能也会得到结果，但结合这个具体的解析式分析一下，可以得到更优的解法。首先，在旋转矩阵R或者降维矩阵B固定其中之一的时候，目标函数是凸函数。故可以考虑对R和B异步优化，即：</p>
<ol>
<li>固定旋转矩阵R，优化降维矩阵B，初始化时R初始为单位阵I即可，直接用普遍的优化方式进行优化；</li>
<li>固定降维矩阵B，优化旋转矩阵R，此时优化目标如下：<script type="math/tex; mode=display">
min_R |R^TY-B^TX|_2^2</script>这个式子有闭式解，对$YX^TB$进行SVD，得到$UDV^T$，便得到$R=UV^T$。</li>
</ol>
<p>重复步骤1，2最终得到优化解。</p>
<h5 id="2-Dimensionality’s-Blessing-Clustering-Images-by-Underlying-Distribution"><a href="#2-Dimensionality’s-Blessing-Clustering-Images-by-Underlying-Distribution" class="headerlink" title="2. Dimensionality’s Blessing: Clustering Images by Underlying Distribution"></a>2. Dimensionality’s Blessing: Clustering Images by Underlying Distribution</h5><p>这个paper在cvpr一众基于深度网络的方法中显得非常与众不同，使用的方法属于 <strong>高维统计(high-dimensional statistics)</strong> 。总体来说，作者推导了一个根据图像的潜在分布来判断图像是否相似，进而完成聚类的算法。<br>图像聚类，不辅助外部监督信息的情况下，严格来说目前学术界做出来的效果其实没有很好，一些相同的场景在转换了相机的倾角之后，得到的图像甚至都会被认为是不同的景物。<br>考虑从思考图像的本质入手。<br>首先，图像来自于相机，产生一个图像的因素包括，相机的在地球中的位置，相机的倾角，天气条件，阳光的猛烈程度，相机中传感器的噪声等等，这些因素视为随机变量，他们各自拥有一个随机分布，从这些分布中采样，综合得到一张具体的图像，故图像是这些分布中的一个具体实例。<br>作者观点是，现在直接将图像进行聚类，或者降维后聚类，或者抽取特征后进行聚类的这种做法，难以做的很好是因为：<br>a. 本质上，得到一种图像的多种分布采样出来的实例，是无法判断出每个量从哪个分布而来，影响程度多少。换句话说，产生图像数据的这些分布其实是重叠的，即数据是chaotic的(chaotic: chaotic is defined as having distributions whose mean separation is significantly smaller than their standard deviation)。如果增大feature的维度，可以减缓这种混合的影响，但是又会导致另一个问题b；<br>b. contrast-loss，即如果一个向量空间维度足够大，则向量空间中的任意两个点的距离是趋向于一个常数值的(part of the curse of dimensionality)，这个可能有点违背直觉。由于高维空间的这个特性，如果为了避免问题a而增大向量维度，则问题可能会朝b方向恶化；</p>
<p>作者的观点是，应当增大向量空间的维度，但是可以避开问题b。相反地，可以利用高维空间的一些特性(所以题目是blessing of dimensionality)。</p>
<p>首先看看需要利用到的高维空间的一些特性。<br>其实高维空间有一些很违背直觉的特性。之所以违背直觉是因为我们生活在三维世界里，习惯了想象一些二维或者三维的物体，而且，超过4维的世界很难直观地可视化出来。例如，在二维的平面世界中，我们直观地看到，一个正方形除去内接圆的部分，占整个正方形面积的比重比内接圆占的比重要小，在三维的立体世界中也是如此，正方体除去求解球的部分，占整个正方体体积的比重比内接球占的比重要小。但实际上，可以定量的计算出，随着二维变到三维，这个残余部分的比重在增大。实际上，这种’面积体积’的概念可以推广到d维，此时，超球体和超立方体的’超体积’的比值计算如下：</p>
<script type="math/tex; mode=display">
\frac{2r^d \pi^{d/2}}{d\Gamma (d/2)} = \frac{\pi^{d/2}}{d 2^{d-1}\Gamma (d/2)}</script><p>从这里可以看到，当d越来越大，这两个超体积的比值变得越来越小，当d趋向于无穷时，这个比值变成了0。这说明，在高维空间里，内接的超球体只占了整个空间的非常微小的一部分，这反映了高维空间实际上非常之空阔。</p>
<p>在paper中，作者的说法是，在一个高维空间中，一个超球体的超体积随半径的变化是非常剧烈的，如下：</p>
<script type="math/tex; mode=display">
(\frac{r-\Delta r}{r})^k = (1-\frac{\Delta r}{r})^k \rightarrow 0, k \rightarrow \infty</script><p>故维度足够高的话，超球体的超体积其实都是超球体的最外层贡献的，几乎和内部无关。因此，如果从这样一个高维超球体范围内中采样一个点，该点基本就是落在球体的表面而已，因为内部空间小到可以忽略，只有无穷小的概率会采样到内部点。关于这个，作者尝试给了一个可视化图，如下：<br><img src="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/2020-02-06-21-17-55.png" class=""><br>从上面结论出发，作者进一步说明了，如果构成图像的所有潜在分布的维度足够高，那么从中采样一个示例出来，其实就是落在这些分布构成的超球体的表面极其‘稀薄’的一圈圆环而已，从这一点来看，一些重叠的高维分布的实例，其实并没有发生混合。示意图如下：<br><img src="/2020/02/06/%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE/2020-02-06-21-18-30.png" class=""><br>另外，因为图像的像素局部是相关的，较远距离的像素是无关的，故无法将像素之间都当做严格独立的， 于是作者又尝试说明了‘准独立’(quasi-independent)的一些特性，即在无限高维的向量中，只有有限维度的向量之间是不独立的情况。</p>
<p>经过一番推理之后，得出结论，在准独立的情况下，高维空间中，两个向量之间的距离几乎取决于所在分布的均值和方差，和向量具体的值无关。所以，对图像聚类，需要判断两个图像向量之间的距离，转化为识别两个图像潜在分布的均值和方差。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangmiaoxin.github.io/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MioKinHuang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MioKinHuang 's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/" itemprop="url">论文阅读：基于对抗网络的无监督生成式文本摘要</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-06T17:40:50+08:00">
                2020-02-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index">
                    <span itemprop="name">论文阅读</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文是对ICLR2018的论文 <strong>《Learning to Encode Text as Human-Readable Summaries using generative adversarial netword》</strong> 的阅读笔记，总结了一下论文的做法，思考了一些可能存在的问题，也分析一下后续可能的技术借鉴和迁移。</p>
<h3 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h3><p>autoencoder常常被用来编码和压缩信息。例如在句子表示中，可以将句子压缩成一个稠密向量，表示空间中某个具体的点。但是，这个表示方法人是无法感知的。作者借鉴autoencoder的思想，提出了一种新的思路，直接将长文本压缩成短文本。这个过程可以完全使用无监督方法。</p>
<h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h3><p>从动机出发，考虑将长文本先经过一个seq2seq模型，压缩为一个短文本，这里的seq2seq模型称为Generator，此处的监督信息来自两部分：</p>
<ol>
<li>短文本经过另一个seq2seq模型，能恢复为长文本，此处的seq2seq模型称为Reconstructor；</li>
<li>很明显，仅仅是重构约束的话，Generator和Reconstructor之间可能创造了另一种奇怪的语言，导致生成的短文本虽然可以重构为长文本但是却不是人类能读懂的语言，故这里需要加另一个约束，即文本必须对人类来说通顺的，考虑增加一个判别器Discriminator，用来对生成的文本的通顺程度打分；<br>总体的模型示意图如下所示：<img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-02.png" class="">
</li>
</ol>
<p>了解了模型的总体结构之后，下面是模型比较详细一点的框架示意图：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-17.png" class=""></p>
<ol>
<li><p>Generator和Reconstructor：都是基于seq2seq结构，原始paper中辅助了attention和copy机制，此处框架示意图没有画出来。这里需要注意的问题是，Reconstructor接收的短文本是由argmax获得的，该函数不可导，故梯度无法传导，按照习惯解法引入强化学习，将Reconstructor端的loss取反，作为reward传导到Generator的短文本生成端作为优化目标。</p>
</li>
<li><p>Discriminator：原始paper提出了两种方法：<br>a. 使用WGAN：假样本是Generator生成的每一个token的概率分布拼接成的矩阵，真样本是真实句子的one-hot向量拼接成的矩阵。使用CNN网络进行编码，使用推土距离(earth mover’s distance)计算一个连续概率分布和一个one-hot分布之间的距离；<br>b. Adversarial REINFORCE：paper中提出的方法。假样本是Generator生成的概率分布进行采样(例如argmax)获得的tokens，真样本是真实句子对应的tokens。使用一个单向的lstm进行编码，每个timestep输入一个token，相应的输出一个分数$s_n$，使用所有N个timestep的分数总和计算$D_{loss}$，如下：</p>
<script type="math/tex; mode=display">
D_2(y^s) = \frac{1}{N} \sum_{n=1}^N s_n
D_{loss} = \frac{1}{K} \sum D_2(y^{s(k)})-\frac{1}{K} \sum _{k=1}^KD_2(y^{real(k)})\\
+\beta _2\frac{1}{K} \sum_{k=1}^K(\Delta _{y^{i(k)}} D_2(y^{i(k)}) -1)^2</script><p>其中$\beta _2$这一项是对梯度做惩罚。可以看到，Discriminator能越早决定句子的真假，则loss会越低。</p>
</li>
</ol>
<h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h3><p>English Gigaword是一个英文摘要常用的数据集，包括3.8百万的doc-summary构成的pair。<br>如下是分别和现有的基于监督学习的工作，自己构建的无监督方法，半监督方法和简单的迁移学习方法的对比。此处的半监督是使用部分的Gigaword数据集的带标签语料，迁移学习是指使用了CNN/Diary数据集的摘要部分的句子作为真样本训练Discriminator。<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-41.png" class=""><br>部分样本结果如下：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-43-52.png" class=""></p>
<p>其他数据集CNN/Daily和Chinese Gigaword的结果，如下：<br><img src="/2020/02/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%AE%BA%E6%96%87-%E5%AF%B9%E5%81%B6%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/2020-02-06-17-44-05.png" class=""></p>
<h2 id="思考与延伸"><a href="#思考与延伸" class="headerlink" title="思考与延伸"></a>思考与延伸</h2><p>paper提出了一种文本摘要的方法，一个非常大的亮点是无监督，并针对文本生成中常遇到的采样问题(例如argmax)设计了一些解法，解法是目前nlp中常用的对付梯度回传问题的做法，即采用强化学习将梯度信息从不可导的地方，用reward的方式间接回传到前端网络。从实验结果看，paper中提出的Adversarial REINFORCE的做法要比过往的WGAN的做法效果好一点。在summary数据集做评估，效果比不上有监督的方法，这是合理的，但是已经非常惊艳了。</p>
<p>从提供的样例看效果也足够惊人，比较大的问题其实在于文中的预训练，RL在动作空间大的情况下难以收敛，nlp问题对应的动作空间大小常常是词典的大小，几十万级别的动作对RL来说是非常大的数目，一般都会引入预训练步骤去获得一个语言模型用来为RL搜索剪除大量无用搜索路径。但论文使用的预训练模式，应该有泄漏数据的问题，因为Adversarial REINFORCE方法中，辨别器的预训练是拿摘要数据训练的，无形之中辨别器有引导生成器去生成真实摘要数据的趋势，因为辨别器见过真数据，只是原文和摘要之间对不上号，但学习些数据特点还是有的。所以这应该不能算纯粹的无监督学习，这监督信号给的比较间接和隐蔽。</p>
<p>当然，总体来说思路非常之优雅，并且，本文的摘要方法可以不仅仅是对摘要，从大的层面讲，是一种文本转换的方法（此处的转换是长到短），只是从无监督出发的话，长短转换是最直接了当的无监督，需要的约束仅仅在于长度，而长度是非常快捷无成本获得的“监督信息”，所以也就是无监督。如果按照大的层面思考一下，其实文本转换不局限于长度，可以有语言格式的转化，例如翻译，语音识别；或者也可以是图片信息和文字信息的转化，图文互转。其中，翻译的思路其实和微软的对偶学习论文雷同，仅在于微软论文没有涉及GAN，强推对偶学习概念，本文阅读的论文是从编码解码中间态的可读性角度出发，避开了对偶学习的概念，但其实只是论文写作技巧的问题，道理是一致的。图文互转的话，容易令人想起的另一个场景是CycleGAN的图图互转，其实这三者内在的思路都是利用闭环带来新的回馈信息，增加新的约束项。</p>
<p>类似的思路，不同的场景和处理技巧，目前看是CycleGAN认可度最高，除去微软论文复现率太低导致诟病之外，也反映出nlp问题和图像问题的不同。CycleGAN面对图像这种天然存在的物理信号，全程可导，闭环不需要特殊处理，本文阅读的论文和微软的论文面对的是文字的转换，文字信息属于人类抽象的符号，不可导，闭环需要借助RL做特殊处理，目前看训练困难而且个人不严谨地估计一下，因为RL探索的动作空间实在太大，效果的提升可能过分依赖于RL预训练的程度，然后在预训练附近做微调和艰难探索，导致这种方法上限其实不高。</p>
<p>沿着这个思路，还有一篇 <strong>NAACL2019</strong> 的论文（SEQ3）继续进行了新的探索。同样是从文本摘要入手，思路同样是归纳为闭环，但处理文字信号的不可导问题引入了新的思路，gumble-softmax，也是贴着无监督的标签，看起来要比本文的方法更站的住脚。同时，因为舍弃NLP生成问题常用的RL的技巧，直接设计了可导的损失函数，道理上讲效果的上限也会更高一些。具体的阅读和思考，就放在下一篇文章再说了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">MioKinHuang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MioKinHuang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
